{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TPUInceptionV3+RNN-TF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "najlB4tnapzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install imgaug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4wTFMNrAZY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/competitions/human-protein-atlas-image-classification/output\n",
        "import os\n",
        "os.chdir('/content/competitions/human-protein-atlas-image-classification')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RVBQEkhE1wFO",
        "colab_type": "code",
        "outputId": "52570f41-d29e-4798-aa09-b5bf5c92476c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.24.81.122:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 17779721942798519834),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14974208131344447166),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2051734382117691682),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8244368593160874389),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4711608759806958056),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3007811163809719381),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12529572917288567259),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16269313086056972315),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14915623603826003088),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 747416940851313804),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 229308682914320321),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5579812379159691080)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a5OO_GRS5PdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "from skimage.transform import resize\n",
        "#from imgaug import augmenters as iaa\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SIZE = 299\n",
        "SEED = 777\n",
        "THRESHOLD = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TopFylv55Pde",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UAu2PQ_PD_fX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GCS access helpers ##\n",
        "Courtesy of https://stackoverflow.com/a/52106361/7724174\n",
        "\n",
        "These functions let us get data from GCS into our notebook."
      ]
    },
    {
      "metadata": {
        "id": "bDZlL-_K5Pdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load dataset info\n",
        "#DIR = '../input/'\n",
        "#DIR='gs://human-protein-atlas-kaggle/'\n",
        "#data = dd.read_csv(DIR+'train.csv')\n",
        "#data = data.compute()\n",
        "\n",
        "DATA_DIR='gs://human-protein-atlas-kaggle/'\n",
        "\n",
        "from tensorflow.python.lib.io import file_io\n",
        "with file_io.FileIO(DATA_DIR+'train.csv', 'r') as f:\n",
        "    data = pd.read_csv(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRMpE3B95Pdr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SHAPE = (299, 299, 3)\n",
        "NUM_CLASSES=28\n",
        "#epochs = 400;\n",
        "epochs = 30\n",
        "#batch_size = 256;\n",
        "VAL_RATIO = .1;\n",
        "DEBUG = False\n",
        "channels = [\"green\", \"blue\", \"red\"]\n",
        "lstmUnits=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ciXtnu0dfPO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data input pipline ##\n",
        "This isn't fully optimized yet, but it's good enough."
      ]
    },
    {
      "metadata": {
        "id": "kAm-HbRHN-U3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TF_DIR=DATA_DIR+'train'\n",
        "DS_DIMS=[512,512]\n",
        "NN_DIMS=[299,299]\n",
        "REC_BUF_SIZE=453762 # This is approximate size for 512x512 images\n",
        "NUM_PARALLEL_CALLS=8 # number of cores in the system\n",
        "class HPADataset:\n",
        "    def __init__(self, shards, aug=True):\n",
        "        self.shards = shards\n",
        "        self.aug = aug\n",
        "    def input_fn(self, params):\n",
        "        batch_size=params['batch_size']\n",
        "        def _parse_function(example_proto):\n",
        "            features = {}\n",
        "            for c in channels:\n",
        "                features[\"image/%s/filename\"%c] = tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "                features[\"image/%s/encoded\"%c] = tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "            features[\"image/label\"] = tf.FixedLenFeature((NUM_CLASSES), tf.float32, default_value=[0]*NUM_CLASSES)\n",
        "            parsed_features = tf.parse_single_example(example_proto, features)\n",
        "            imgs=[]\n",
        "            for c in channels:\n",
        "                img=parsed_features['image/%s/encoded'%c]\n",
        "                #print(img)\n",
        "                img=tf.image.decode_png(img, channels=1)\n",
        "                shape=tf.shape(img)\n",
        "                #shape_print=tf.print(shape)\n",
        "                img=tf.reshape(img, DS_DIMS)\n",
        "                imgs.append(img)\n",
        "            image=tf.stack(imgs, axis=-1, name='combine_channels')\n",
        "            image=tf.image.resize_images(image, NN_DIMS)\n",
        "            # For simplicity, we'll use imgaug with py_op here\n",
        "            def augment(image):\n",
        "                augment_img = iaa.Sequential([\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Affine(rotate=0),\n",
        "                        iaa.Affine(rotate=90),\n",
        "                        iaa.Affine(rotate=180),\n",
        "                        iaa.Affine(rotate=270),\n",
        "                        iaa.Fliplr(0.5),\n",
        "                        iaa.Flipud(0.5),\n",
        "                    ])], random_order=True)\n",
        "\n",
        "                image_aug = augment_img.augment_image(image)\n",
        "                return image_aug\n",
        "            if self.aug:\n",
        "                image=tf.py_func(augment, [image], tf.float32, name='augment')\n",
        "                image=tf.reshape(image, NN_DIMS+[len(channels)])\n",
        "            image=tf.cast(image, tf.float32)\n",
        "            image=image / 255.\n",
        "            return image, parsed_features[\"image/label\"]\n",
        "        fnames=['{dir}/hpa_{w}x{h}_{num}.tfrecords'.format(dir=TF_DIR, w=DS_DIMS[0], h=DS_DIMS[1], num=shard) for shard in self.shards]\n",
        "        dataset=tf.data.TFRecordDataset(fnames,\n",
        "                                        buffer_size=REC_BUF_SIZE*2*len(self.shards),\n",
        "                                        num_parallel_reads=len(self.shards))\n",
        "        dataset=dataset.map(_parse_function, num_parallel_calls=NUM_PARALLEL_CALLS)\n",
        "        dataset=dataset.shuffle(1000)\n",
        "        dataset=dataset.prefetch(batch_size*8)\n",
        "        dataset=dataset.batch(batch_size, drop_remainder=True)\n",
        "        dataset=dataset.prefetch(2)\n",
        "        if params['mode']!='predict':\n",
        "            dataset=dataset.repeat()\n",
        "            return dataset.make_one_shot_iterator().get_next()\n",
        "        return dataset\n",
        "#with tf.Graph().as_default():\n",
        "#    test=HPADataset([1,2]).input_fn()\n",
        "#    with tf.Session() as sess:\n",
        "#        sess.run(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qX02e8aPkb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_VNMbwOsvV-4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tg = HPADataset(range(8), False)\n",
        "vg = HPADataset([8, 9], False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoxbV4Hsd1jA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model ##\n",
        "\n",
        "What follows is our model based on pure tensorflow and the inception model present there. Large portions of this code are lifted from https://github.com/tensorflow/tpu/blob/master/models/experimental/inception/inception_v3.py\n"
      ]
    },
    {
      "metadata": {
        "id": "kD7NGt2Y5Pdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import summary\n",
        "from tensorflow.contrib.framework.python.ops import arg_scope\n",
        "from tensorflow.contrib.slim.nets import inception\n",
        "from tensorflow.contrib.training.python.training import evaluation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k3QEuM7qf4rp",
        "colab_type": "code",
        "outputId": "efca3045-fec4-4307-ce1e-2d750010ad2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "cell_type": "code",
      "source": [
        "### Some model settings\n",
        "precision='float32'\n",
        "log_device_placement=False\n",
        "clear_update_collections=True\n",
        "num_classes=NUM_CLASSES\n",
        "display_tensors=True\n",
        "use_tpu=True\n",
        "train_batch_size=1024\n",
        "geval_batch_size=1024\n",
        "glearning_rate=0.165\n",
        "learning_rate_decay=0.94\n",
        "use_learning_rate_warmup=False\n",
        "warmup_epochs=7\n",
        "cold_epochs=2\n",
        "learning_rate_decay_epochs=6\n",
        "skip_host_call=True\n",
        "goptimizer='RMS'\n",
        "moving_average=True\n",
        "MOVING_AVERAGE_DECAY = 0.995\n",
        "# Batchnorm moving mean/variance parameters\n",
        "BATCH_NORM_DECAY = 0.996\n",
        "BATCH_NORM_EPSILON = 1e-3\n",
        "\n",
        "WEIGHT_DECAY = 0.00004\n",
        "RMSPROP_DECAY = 0.9                # Decay term for RMSProp.\n",
        "RMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\n",
        "RMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n",
        "\n",
        "min_depth=16\n",
        "transpose_enabled=False\n",
        "spatial_squeeze=True\n",
        "\n",
        "_NUM_TRAIN_IMAGES = 24858\n",
        "_NUM_EVAL_IMAGES = 6214\n",
        "epochs=30\n",
        "ITERATIONS=_NUM_TRAIN_IMAGES*epochs/train_batch_size\n",
        "num_shards=8 # 8 in original..\n",
        "#model_dir='/content/competitions/human-protein-atlas-image-classification/output'\n",
        "model_dir=DATA_DIR+'output/'\n",
        "save_checkpoints_secs=1000\n",
        "save_summary_steps=100\n",
        "eval_timeout=None\n",
        "train_steps_per_eval=int(_NUM_TRAIN_IMAGES/train_batch_size) # essentially one epoch\n",
        "\n",
        "dropout_keep_prob=0.8\n",
        "train_steps=int(ITERATIONS)\n",
        "print('Will train for {train} steps'.format(train=train_steps))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Will train for 728 steps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9NYAVMEH45rM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def tensor_transform_fn(data, perm):\n",
        "  \"\"\"Transpose function.\n",
        "\n",
        "  This function is used to transpose an image tensor on the host and then\n",
        "  perform an inverse transpose on the TPU. The transpose on the TPU gets\n",
        "  effectively elided thus voiding any associated computational cost.\n",
        "\n",
        "  NOTE: Eventually the compiler will be able to detect when this kind of\n",
        "  operation may prove beneficial and perform these types of transformations\n",
        "  implicitly, voiding the need for user intervention\n",
        "\n",
        "  Args:\n",
        "    data: Tensor to be transposed\n",
        "    perm: New ordering of dimensions\n",
        "\n",
        "  Returns:\n",
        "    Transposed tensor\n",
        "  \"\"\"\n",
        "  if transpose_enabled:\n",
        "    return tf.transpose(data, perm)\n",
        "  return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Up9dhjFpe-14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib import layers\n",
        "from tensorflow.contrib.layers.python.layers import layers as layers_lib\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def inception_model_fn(features, labels, mode, params):\n",
        "    \"\"\"Inception v3 model using Estimator API.\"\"\"\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n",
        "\n",
        "    if isinstance(features, dict):\n",
        "        features = features['feature']\n",
        "\n",
        "    features = tensor_transform_fn(features, params['input_perm'])\n",
        "\n",
        "    # This nested function allows us to avoid duplicating the logic which\n",
        "    # builds the network, for different values of --precision.\n",
        "    def build_inception_v3(final_endpoint='Mixed_7c', scope=None):\n",
        "        with tf.variable_scope(\n",
        "            scope, 'InceptionV3', [features, num_classes], reuse=None) as scope:\n",
        "            with arg_scope(\n",
        "              [layers_lib.batch_norm, layers_lib.dropout], is_training=is_training):\n",
        "              net, end_points = inception.inception_v3_base(\n",
        "                  features,\n",
        "                  final_endpoint=final_endpoint\n",
        "              )\n",
        "\n",
        "              # Build our RNNs\n",
        "              def build_rnn(endpoint):\n",
        "                # 1. Unroll our logits horisonatlly and vertically\n",
        "                #print(\"Input endpoint\", endpoint)\n",
        "                shape=endpoint.shape.as_list()\n",
        "                shape=(shape[0], shape[1]*shape[2], shape[3])\n",
        "                #print(\"target shape\", shape)\n",
        "                input_width=tf.reshape(endpoint, shape)\n",
        "                input_height=tf.transpose(endpoint, (0, 2, 1, 3))\n",
        "                input_height=tf.reshape(input_height, shape)\n",
        "                #print(\"Input\", input_height)\n",
        "                lstmCellW = tf.contrib.rnn.LSTMCell(num_units=lstmUnits, use_peepholes=True)\n",
        "                lstmCellH = tf.contrib.rnn.LSTMCell(num_units=lstmUnits, use_peepholes=True)\n",
        "                wouts, wstate=tf.nn.dynamic_rnn(lstmCellW, input_width, scope='width', dtype=tf.float32, parallel_iterations=1024)\n",
        "                houts, hstate=tf.nn.dynamic_rnn(lstmCellH, input_height, scope='height', dtype=tf.float32, parallel_iterations=1024)\n",
        "                #print(\"RNN outs\", wouts, houts)\n",
        "                #print(\"Last of wouts\", wouts[-1])\n",
        "                #wouts=tf.unstack(wouts, axis=1)\n",
        "                #houts=tf.unstack(houts, axis=1)\n",
        "                #print(\"Unstacked\", wouts)\n",
        "                #print(\"Last?\", wouts[-1])\n",
        "                #wouts=wouts[-1]\n",
        "                #houts=houts[-1]\n",
        "                #logits=tf.concat([houts, wouts], name='Logits', axis=-1)\n",
        "                #print(houts, wouts, logits)\n",
        "                logits=tf.concat([wstate.c, hstate.c], name='Logits', axis=-1)\n",
        "                return logits\n",
        "              rnn_logits=[]\n",
        "              #attachements=['Mixed_7b', 'Mixed_7a', 'Mixed_6e']\n",
        "              attachements=['Mixed_7a']\n",
        "              #attachements=[]\n",
        "              for att_pt in attachements:\n",
        "                    with tf.variable_scope(att_pt+'_rnn'):\n",
        "                        rnn_logits.append(build_rnn(end_points[att_pt]))\n",
        "              #print(rnn_logits)\n",
        "              rnn_logits=tf.concat(rnn_logits, name='FinalRNNLogits', axis=-1)\n",
        "\n",
        "              depth = lambda d: max(d, min_depth)\n",
        "              if 'Mixed_6e' in end_points:\n",
        "                  # Auxiliary Head logits\n",
        "                  with arg_scope(\n",
        "                      [layers.conv2d, layers_lib.max_pool2d, layers_lib.avg_pool2d],\n",
        "                      stride=1,\n",
        "                      padding='SAME'):\n",
        "                    aux_logits = end_points['Mixed_6e']\n",
        "                    with tf.variable_scope('AuxLogits'):\n",
        "                      aux_logits = layers_lib.avg_pool2d(\n",
        "                          aux_logits, [5, 5],\n",
        "                          stride=3,\n",
        "                          padding='VALID',\n",
        "                          scope='AvgPool_1a_5x5')\n",
        "                      aux_logits = layers.conv2d(\n",
        "                          aux_logits, depth(128), [1, 1], scope='Conv2d_1b_1x1')\n",
        "\n",
        "                      # Shape of feature map before the final layer.\n",
        "                      kernel_size = [5, 5]\n",
        "                      aux_logits = layers.conv2d(\n",
        "                          aux_logits,\n",
        "                          depth(768),\n",
        "                          kernel_size,\n",
        "                          weights_initializer=tf.initializers.truncated_normal(stddev=0.01),\n",
        "                          padding='VALID',\n",
        "                          scope='Conv2d_2a_{}x{}'.format(*kernel_size))\n",
        "                      aux_logits = layers.conv2d(\n",
        "                          aux_logits,\n",
        "                          num_classes, [1, 1],\n",
        "                          activation_fn=None,\n",
        "                          normalizer_fn=None,\n",
        "                          weights_initializer=tf.initializers.truncated_normal(stddev=0.001),\n",
        "                          scope='Conv2d_2b_1x1')\n",
        "                      if spatial_squeeze:\n",
        "                        aux_logits = array_ops.squeeze(\n",
        "                            aux_logits, name='SpatialSqueeze')\n",
        "                      end_points['AuxLogits'] = aux_logits\n",
        "            # Final pooling and prediction\n",
        "            with tf.variable_scope('Logits'):\n",
        "                #kernel_size = [8,8]\n",
        "                #net = layers_lib.avg_pool2d(net, kernel_size, padding='VALID', scope='AvgPool_1a{}x{}'.format(*kernel_size))\n",
        "                ## 1x1x2048\n",
        "                #net = layers_lib.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
        "                #end_points['PreLogits']=net\n",
        "                ##2048\n",
        "                #logits = layers.conv2d(net, num_classes, [1,1], activation_fn=None, normalizer_fn=None, scope='Conv2d_1c_1x1')\n",
        "                #if spatial_squeeze:\n",
        "                #    logits = array_ops.squeeze(logits, [1,2], name='SpatialSqueeze')\n",
        "                #end_points['Logits']=logits\n",
        "                #end_points['Predictions']=tf.nn.sigmoid(logits, name='Predicitons')\n",
        "                #return logits, end_points\n",
        "\n",
        "                net = rnn_logits\n",
        "                end_points['PreLogits']=net\n",
        "                end_points['rnn_logits']=net\n",
        "                net = layers_lib.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
        "                print(net)\n",
        "                net = tf.expand_dims(net, 1)\n",
        "                net = tf.expand_dims(net, 1)\n",
        "                print(net)\n",
        "                logits = layers.conv2d(net, num_classes, 1, activation_fn=None, normalizer_fn=None, scope='Conv2d_1c_1x1')\n",
        "                if spatial_squeeze:\n",
        "                    logits = array_ops.squeeze(logits, [1,2], name='SpatialSqueeze')\n",
        "                # 28\n",
        "                end_points['Logits']=logits\n",
        "                end_points['Predictions'] = tf.nn.sigmoid(logits, name='Predictions')\n",
        "                return logits, end_points\n",
        "\n",
        "    def build_network(precision):\n",
        "        if precision == 'bfloat16':\n",
        "            with tf.contrib.tpu.bfloat16_scope():\n",
        "                logits, end_points = build_inception_v3()\n",
        "            logits = tf.cast(logits, tf.float32)\n",
        "        elif precision == 'float32':\n",
        "            logits, end_points = build_inception_v3()\n",
        "        return logits, end_points\n",
        "\n",
        "    if clear_update_collections:\n",
        "        # updates_collections must be set to None in order to use fused batchnorm\n",
        "        with arg_scope(inception.inception_v3_arg_scope(\n",
        "            weight_decay=0.0,\n",
        "            batch_norm_decay=BATCH_NORM_DECAY,\n",
        "            batch_norm_epsilon=BATCH_NORM_EPSILON,\n",
        "            updates_collections=None)):\n",
        "            logits, end_points = build_network('float32')\n",
        "    else:\n",
        "        with arg_scope(inception.inception_v3_arg_scope(\n",
        "            batch_norm_decay=BATCH_NORM_DECAY,\n",
        "            batch_norm_epsilon=BATCH_NORM_EPSILON)):\n",
        "            logits, end_points = build_network('float32')\n",
        "\n",
        "    predictions = {\n",
        "        'logits': logits,\n",
        "        'classes': tf.math.greater(logits, 0.2),\n",
        "        'probabilities': end_points['Predictions'],\n",
        "        #'rnn_logits': end_points['rnn_logits'],\n",
        "        #'dropout': end_points['dropout'],\n",
        "        'prelogits': end_points['PreLogits'],\n",
        "        'labels': labels,\n",
        "    }\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return  tf.contrib.tpu.TPUEstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions=predictions,\n",
        "            export_outputs={\n",
        "                'classify': tf.estimator.export.PredictOutput(predictions)\n",
        "            })\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.EVAL and display_tensors and (\n",
        "        not use_tpu):\n",
        "        with tf.control_dependencies([\n",
        "            tf.Print(\n",
        "                predictions['classes'], [predictions['classes']],\n",
        "                summarize=geval_batch_size,\n",
        "                message='prediction: ')\n",
        "        ]):\n",
        "            labels = tf.Print(\n",
        "                labels, [labels], summarize=geval_batch_size, message='label: ')\n",
        "\n",
        "    # in our case labels come pre-encoded\n",
        "    one_hot_labels = labels #tf.one_hot(labels, num_classes, dtype=tf.int32)\n",
        "\n",
        "    if 'AuxLogits' in end_points:\n",
        "        tf.losses.sigmoid_cross_entropy(\n",
        "            multi_class_labels=one_hot_labels,\n",
        "            logits=tf.cast(end_points['AuxLogits'], tf.float32),\n",
        "            weights=0.4,\n",
        "            label_smoothing=0.1,\n",
        "            scope='aux_loss')\n",
        "\n",
        "    tf.losses.sigmoid_cross_entropy(\n",
        "        multi_class_labels=one_hot_labels,\n",
        "        logits=logits,\n",
        "        #weights=1.0,\n",
        "        #label_smoothing=0.1,\n",
        "        reduction=tf.losses.Reduction.MEAN\n",
        "    )\n",
        "\n",
        "    losses = tf.add_n(tf.losses.get_losses())\n",
        "    l2_loss = []\n",
        "    for v in tf.trainable_variables():\n",
        "        if 'BatchNorm' not in v.name and 'weights' in v.name:\n",
        "            l2_loss.append(tf.nn.l2_loss(v))\n",
        "    loss = losses + WEIGHT_DECAY * tf.add_n(l2_loss)\n",
        "\n",
        "    initial_learning_rate = glearning_rate * train_batch_size / 256\n",
        "    if use_learning_rate_warmup:\n",
        "        # Adjust initial learning rate to match final warmup rate\n",
        "        warmup_decay = learning_rate_decay**(\n",
        "            (warmup_epochs + cold_epochs) /\n",
        "            learning_rate_decay_epochs)\n",
        "        adj_initial_learning_rate = initial_learning_rate * warmup_decay\n",
        "\n",
        "    final_learning_rate = 0.0001 * initial_learning_rate\n",
        "\n",
        "    host_call = None\n",
        "    train_op = None\n",
        "  \n",
        "    if is_training:\n",
        "        batches_per_epoch = _NUM_TRAIN_IMAGES / train_batch_size\n",
        "        global_step = tf.train.get_or_create_global_step()\n",
        "        current_epoch = tf.cast(\n",
        "            (tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n",
        "\n",
        "        learning_rate = tf.train.exponential_decay(\n",
        "            learning_rate=initial_learning_rate,\n",
        "            global_step=global_step,\n",
        "            decay_steps=int(learning_rate_decay_epochs * batches_per_epoch),\n",
        "            decay_rate=learning_rate_decay,\n",
        "            staircase=True)\n",
        "\n",
        "        if use_learning_rate_warmup:\n",
        "            wlr = 0.1 * adj_initial_learning_rate\n",
        "            wlr_height = tf.cast(\n",
        "                0.9 * adj_initial_learning_rate /\n",
        "                (warmup_epochs + learning_rate_decay_epochs - 1),\n",
        "                tf.float32)\n",
        "            epoch_offset = tf.cast(cold_epochs - 1, tf.int32)\n",
        "            exp_decay_start = (warmup_epochs + cold_epochs +\n",
        "                             learning_rate_decay_epochs)\n",
        "            lin_inc_lr = tf.add(\n",
        "                wlr, tf.multiply(\n",
        "                    tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32),\n",
        "                    wlr_height))\n",
        "            learning_rate = tf.where(\n",
        "                tf.greater_equal(current_epoch, cold_epochs),\n",
        "                (tf.where(tf.greater_equal(current_epoch, exp_decay_start),\n",
        "                          learning_rate, lin_inc_lr)),\n",
        "                wlr)\n",
        "\n",
        "        # Set a minimum boundary for the learning rate.\n",
        "        learning_rate = tf.maximum(\n",
        "            learning_rate, final_learning_rate, name='learning_rate')\n",
        "\n",
        "        if goptimizer == 'sgd':\n",
        "            tf.logging.info('Using SGD optimizer')\n",
        "            optimizer = tf.train.GradientDescentOptimizer(\n",
        "                learning_rate=learning_rate)\n",
        "        elif goptimizer == 'momentum':\n",
        "            tf.logging.info('Using Momentum optimizer')\n",
        "            optimizer = tf.train.MomentumOptimizer(\n",
        "                learning_rate=learning_rate, momentum=0.9)\n",
        "        elif goptimizer == 'RMS':\n",
        "            tf.logging.info('Using RMS optimizer')\n",
        "            optimizer = tf.train.RMSPropOptimizer(\n",
        "                learning_rate,\n",
        "                RMSPROP_DECAY,\n",
        "                momentum=RMSPROP_MOMENTUM,\n",
        "                epsilon=RMSPROP_EPSILON)\n",
        "        else:\n",
        "            tf.logging.fatal('Unknown optimizer:', optimizer)\n",
        "\n",
        "        if use_tpu:\n",
        "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        if params['warmup']:\n",
        "            trainable_vars = tf.contrib.framework.get_model_variables()\n",
        "            var_list=tf.contrib.framework.filter_variables(trainable_vars, exclude_patterns='.*Mixed_.*_rnn.*')\n",
        "            trainable_vars = tf.contrib.framework.get_trainable_variables()\n",
        "            var_list=tf.contrib.framework.filter_variables(trainable_vars, exclude_patterns=var_list)\n",
        "            print('Training', var_list)\n",
        "        else:\n",
        "            var_list=None\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            train_op = optimizer.minimize(loss, global_step=global_step, var_list=var_list)\n",
        "        if moving_average:\n",
        "            ema = tf.train.ExponentialMovingAverage(\n",
        "                decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\n",
        "            variables_to_average = (\n",
        "                tf.trainable_variables() + tf.moving_average_variables())\n",
        "            with tf.control_dependencies([train_op]), tf.name_scope('moving_average'):\n",
        "                train_op = ema.apply(variables_to_average)\n",
        "\n",
        "        # To log the loss, current learning rate, and epoch for Tensorboard, the\n",
        "        # summary op needs to be run on the host CPU via host_call. host_call\n",
        "        # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n",
        "        # dimension. These Tensors are implicitly concatenated to\n",
        "        # [params['batch_size']].\n",
        "        gs_t = tf.reshape(global_step, [1])\n",
        "        loss_t = tf.reshape(loss, [1])\n",
        "        lr_t = tf.reshape(learning_rate, [1])\n",
        "        ce_t = tf.reshape(current_epoch, [1])\n",
        "\n",
        "        if not skip_host_call:\n",
        "            def host_call_fn(gs, loss, lr, ce):\n",
        "                \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "                This function is executed on the CPU and should not directly reference\n",
        "                any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "                model to the `metric_fn`, provide them as part of the `host_call`. See\n",
        "                https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "                for more information.\n",
        "                Arguments should match the list of `Tensor` objects passed as the second\n",
        "                element in the tuple passed to `host_call`.\n",
        "                Args:\n",
        "                  gs: `Tensor with shape `[batch]` for the global_step\n",
        "                  loss: `Tensor` with shape `[batch]` for the training loss.\n",
        "                  lr: `Tensor` with shape `[batch]` for the learning_rate.\n",
        "                  ce: `Tensor` with shape `[batch]` for the current_epoch.\n",
        "                Returns:\n",
        "                  List of summary ops to run on the CPU host.\n",
        "                \"\"\"\n",
        "                gs = gs[0]\n",
        "                with summary.create_file_writer(model_dir).as_default():\n",
        "                    with summary.always_record_summaries():\n",
        "                        summary.scalar('loss', tf.reduce_mean(loss), step=gs)\n",
        "                        summary.scalar('learning_rate', tf.reduce_mean(lr), step=gs)\n",
        "                        summary.scalar('current_epoch', tf.reduce_mean(ce), step=gs)\n",
        "\n",
        "                    return summary.all_summary_ops()\n",
        "\n",
        "            host_call = (host_call_fn, [gs_t, loss_t, lr_t, ce_t])\n",
        "\n",
        "    eval_metrics = None\n",
        "    if is_eval:\n",
        "        def metric_fn(labels, logits):\n",
        "            \"\"\"Evaluation metric function. Evaluates accuracy.\n",
        "            This function is executed on the CPU and should not directly reference\n",
        "            any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n",
        "            to the `metric_fn`, provide as part of the `eval_metrics`. See\n",
        "            https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "            for more information.\n",
        "            Arguments should match the list of `Tensor` objects passed as the second\n",
        "            element in the tuple passed to `eval_metrics`.\n",
        "            Args:\n",
        "            labels: `Tensor` with shape `[batch, ]`.\n",
        "            logits: `Tensor` with shape `[batch, num_classes]`.\n",
        "            Returns:\n",
        "            A dict of the metrics to return from evaluation.\n",
        "            \"\"\"\n",
        "            probs=tf.nn.sigmoid(logits)\n",
        "            predictions = tf.math.greater(probs, 0.2)\n",
        "            recall = tf.metrics.recall(labels, predictions)\n",
        "            precision=tf.metrics.precision(labels, predictions)\n",
        "            f1=tf.contrib.metrics.f1_score(labels, probs)\n",
        "\n",
        "            return {\n",
        "              'recall': recall,\n",
        "              'precision': precision,\n",
        "              'f1': f1\n",
        "            }\n",
        "\n",
        "        eval_metrics = (metric_fn, [labels, logits])\n",
        "\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "        mode=mode,\n",
        "        loss=loss,\n",
        "        train_op=train_op,\n",
        "        host_call=host_call,\n",
        "        eval_metrics=eval_metrics)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4fSZX5_Tv44y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LoadEMAHook(tf.train.SessionRunHook):\n",
        "  \"\"\"Hook to load exponential moving averages into corresponding variables.\"\"\"\n",
        "\n",
        "  def __init__(self, model_dir):\n",
        "    super(LoadEMAHook, self).__init__()\n",
        "    self._model_dir = model_dir\n",
        "\n",
        "  def begin(self):\n",
        "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
        "    variables_to_restore = ema.variables_to_restore()\n",
        "    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n",
        "        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n",
        "\n",
        "  def after_create_session(self, sess, coord):\n",
        "    tf.logging.info('Reloading EMA...')\n",
        "    self._load_ema(sess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atKLBN6BwJaZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def do_it(mode):\n",
        "  params = {\n",
        "      'input_perm': [0, 1, 2, 3],\n",
        "      'output_perm': [0, 1, 2, 3],\n",
        "      'warmup': mode=='warmup',\n",
        "      'mode': mode,\n",
        "  }\n",
        "\n",
        "  if mode == 'retrain':\n",
        "    # wipe checkpoints\n",
        "    files=tf.gfile.ListDirectory(model_dir)\n",
        "    for f in files:\n",
        "        fname=model_dir+f\n",
        "        fs = tf.gfile.Stat(fname)\n",
        "        if not fs.is_directory:\n",
        "            tf.gfile.Remove(fname)\n",
        "\n",
        "    do_it('warmup')\n",
        "    do_it('train')\n",
        "  if mode == 'warmup':\n",
        "    mode = 'train'\n",
        "  if mode == 'retrain_and_eval':\n",
        "    # wipe checkpoints\n",
        "    files=tf.gfile.ListDirectory(model_dir)\n",
        "    for f in files:\n",
        "        fname=model_dir+f\n",
        "        fs = tf.gfile.Stat(fname)\n",
        "        if not fs.is_directory:\n",
        "            tf.gfile.Remove(fname)\n",
        "    do_it('warmup')\n",
        "    do_it('train_and_eval')\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  tf.logging.info('Precision: %s', precision)\n",
        "\n",
        "  if mode == 'predict':\n",
        "        batch_axis=None\n",
        "  else:\n",
        "    batch_axis = 0\n",
        "    if transpose_enabled:\n",
        "        params['input_perm'] = [3, 0, 1, 2]\n",
        "        params['output_perm'] = [1, 2, 3, 0]\n",
        "        batch_axis = 3\n",
        "    batch_axis=(batch_axis, 0)\n",
        "\n",
        "  eval_size = _NUM_EVAL_IMAGES\n",
        "  eval_steps = eval_size // geval_batch_size\n",
        "\n",
        "  iterations = (eval_steps if mode == 'eval' else save_summary_steps)\n",
        "\n",
        "  eval_batch_size = (None if mode == 'train' else geval_batch_size)\n",
        "\n",
        "  per_host_input_for_training = (num_shards <= 8 if mode == 'train' else True)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=model_dir,\n",
        "      save_checkpoints_secs=save_checkpoints_secs,\n",
        "      save_summary_steps=save_summary_steps,\n",
        "      session_config=tf.ConfigProto(\n",
        "          allow_soft_placement=True,\n",
        "          log_device_placement=log_device_placement),\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=iterations,\n",
        "          num_shards=num_shards,\n",
        "          per_host_input_for_training=per_host_input_for_training))\n",
        "\n",
        "  trainable_vars = tf.contrib.framework.get_model_variables()\n",
        "  #print(trainable_vars)\n",
        "  skip_vars=['InceptionV3/AuxLogits/Conv2d_2b_1x1/weights']\n",
        "  load_vars = tf.contrib.framework.filter_variables(trainable_vars, exclude_patterns=skip_vars)\n",
        "  #print(load_vars)\n",
        "  ws = tf.estimator.WarmStartSettings(\n",
        "      ckpt_to_initialize_from=DATA_DIR+\"pre-trained/inception_v3.ckpt\",\n",
        "      vars_to_warm_start=load_vars\n",
        "  )\n",
        "  inception_classifier = tf.contrib.tpu.TPUEstimator(\n",
        "      model_fn=inception_model_fn,\n",
        "      use_tpu=use_tpu,\n",
        "      config=run_config,\n",
        "      warm_start_from=ws,\n",
        "      params=params,\n",
        "      train_batch_size=train_batch_size,\n",
        "      eval_batch_size=eval_batch_size,\n",
        "      predict_batch_size=eval_batch_size,\n",
        "      batch_axis=batch_axis)\n",
        "\n",
        "  # Input pipelines are slightly different (with regards to shuffling and\n",
        "  # preprocessing) between training and evaluation.\n",
        "  use_bfloat16 = precision == 'bfloat16'\n",
        "  imagenet_train = tg\n",
        "  imagenet_eval = vg\n",
        "\n",
        "  if moving_average:\n",
        "    eval_hooks = [LoadEMAHook(model_dir)]\n",
        "  else:\n",
        "    eval_hooks = []\n",
        "\n",
        "  if mode == 'eval':\n",
        "    # Run evaluation when there is a new checkpoint\n",
        "    for checkpoint in evaluation.checkpoints_iterator(\n",
        "        model_dir, timeout=eval_timeout):\n",
        "      tf.logging.info('Starting to evaluate.')\n",
        "      try:\n",
        "        start_timestamp = time.time()  # Includes compilation time\n",
        "        eval_results = inception_classifier.evaluate(\n",
        "            input_fn=imagenet_eval.input_fn,\n",
        "            steps=eval_steps,\n",
        "            hooks=eval_hooks,\n",
        "            checkpoint_path=checkpoint)\n",
        "        elapsed_time = int(time.time() - start_timestamp)\n",
        "        tf.logging.info(\n",
        "            'Eval results: %s. Elapsed seconds: %d', eval_results, elapsed_time)\n",
        "\n",
        "        # Terminate eval job when final checkpoint is reached\n",
        "        current_step = int(os.path.basename(checkpoint).split('-')[1])\n",
        "        if current_step >= train_steps:\n",
        "          tf.logging.info(\n",
        "              'Evaluation finished after training step %d', current_step)\n",
        "          break\n",
        "      except tf.errors.NotFoundError:\n",
        "        # Since the coordinator is on a different job than the TPU worker,\n",
        "        # sometimes the TPU worker does not finish initializing until long after\n",
        "        # the CPU job tells it to start evaluating. In this case, the checkpoint\n",
        "        # file could have been deleted already.\n",
        "        tf.logging.info(\n",
        "            'Checkpoint %s no longer exists, skipping checkpoint', checkpoint)\n",
        "\n",
        "  elif mode == 'train_and_eval':\n",
        "    for cycle in range(train_steps // train_steps_per_eval):\n",
        "      tf.logging.info('Starting training cycle %d.' % cycle)\n",
        "      inception_classifier.train(\n",
        "          input_fn=imagenet_train.input_fn, steps=train_steps_per_eval)\n",
        "\n",
        "      tf.logging.info('Starting evaluation cycle %d .' % cycle)\n",
        "      eval_results = inception_classifier.evaluate(\n",
        "          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n",
        "      tf.logging.info('Evaluation results: %s' % eval_results)\n",
        "  elif mode == 'predict':\n",
        "    result=inception_classifier.predict(input_fn=imagenet_eval.input_fn)\n",
        "    for i, r in enumerate(result):\n",
        "        print(r)\n",
        "        if i>50:\n",
        "            break\n",
        "  else:\n",
        "    tf.logging.info('Starting training ...')\n",
        "    if params['warmup']:\n",
        "        steps = train_steps_per_eval*2 # ~2 epochs\n",
        "        print('warming up for ', steps)\n",
        "    else:\n",
        "        steps = train_steps\n",
        "        print('training for ')\n",
        "    inception_classifier.train(\n",
        "        input_fn=imagenet_train.input_fn, steps=steps)\n",
        "\n",
        "  #if export_dir is not None:\n",
        "  #  tf.logging.info('Starting to export model.')\n",
        "  #  inception_classifier.export_saved_model(\n",
        "  #      export_dir_base=export_dir,\n",
        "  #      serving_input_receiver_fn=image_serving_input_fn)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrRGqcnH5Pem",
        "colab_type": "code",
        "outputId": "84cecdb3-d51e-4ebc-d8ec-5cb90879604b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5677
        }
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "#tf.logging.set_verbosity(tf.logging.INFO)\n",
        "do_it('retrain')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Precision: float32\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.24.81.122:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f99c9caed30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.24.81.122:8470', '_evaluation_master': b'grpc://10.24.81.122:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f99c9c92160>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Starting training ...\n",
            "warming up for  48\n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.24.81.122:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 17779721942798519834)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14974208131344447166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2051734382117691682)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8244368593160874389)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4711608759806958056)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3007811163809719381)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12529572917288567259)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16269313086056972315)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14915623603826003088)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 747416940851313804)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 229308682914320321)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5579812379159691080)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(128, 128), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(128, 1, 1, 128), dtype=float32)\n",
            "INFO:tensorflow:Using RMS optimizer\n",
            "Training [<tf.Variable 'InceptionV3/InceptionV3/Conv2d_1a_3x3/weights:0' shape=(3, 3, 3, 32) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_1a_3x3/BatchNorm/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_2a_3x3/weights:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_2a_3x3/BatchNorm/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_2b_3x3/weights:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_2b_3x3/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_3b_1x1/weights:0' shape=(1, 1, 64, 80) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_3b_1x1/BatchNorm/beta:0' shape=(80,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_4a_3x3/weights:0' shape=(3, 3, 80, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Conv2d_4a_3x3/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 192, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 192, 48) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(48,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights:0' shape=(5, 5, 48, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 192, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 192, 32) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 256, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights:0' shape=(1, 1, 256, 48) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(48,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights:0' shape=(5, 5, 48, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 256, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 256, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 288, 48) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(48,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights:0' shape=(5, 5, 48, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights:0' shape=(3, 3, 288, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta:0' shape=(96,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 128) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights:0' shape=(1, 7, 128, 128) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights:0' shape=(7, 1, 128, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 128) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights:0' shape=(7, 1, 128, 128) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights:0' shape=(1, 7, 128, 128) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights:0' shape=(7, 1, 128, 128) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights:0' shape=(1, 7, 128, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights:0' shape=(7, 1, 160, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights:0' shape=(1, 7, 160, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights:0' shape=(7, 1, 160, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0' shape=(160,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights:0' shape=(1, 7, 160, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights:0' shape=(3, 3, 192, 320) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta:0' shape=(320,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights:0' shape=(3, 3, 192, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 1280, 320) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(320,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 1280, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 1280, 448) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(448,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights:0' shape=(3, 3, 448, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 1280, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights:0' shape=(1, 1, 2048, 320) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(320,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights:0' shape=(1, 1, 2048, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights:0' shape=(1, 1, 2048, 448) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0' shape=(448,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights:0' shape=(3, 3, 448, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta:0' shape=(384,) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights:0' shape=(1, 1, 2048, 192) dtype=float32>, <tf.Variable 'InceptionV3/InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0' shape=(192,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/width/lstm_cell/kernel:0' shape=(1344, 256) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/width/lstm_cell/bias:0' shape=(256,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/width/lstm_cell/w_f_diag:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/width/lstm_cell/w_i_diag:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/width/lstm_cell/w_o_diag:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/height/lstm_cell/kernel:0' shape=(1344, 256) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/height/lstm_cell/bias:0' shape=(256,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/height/lstm_cell/w_f_diag:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/height/lstm_cell/w_i_diag:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/Mixed_7a_rnn/height/lstm_cell/w_o_diag:0' shape=(64,) dtype=float32>, <tf.Variable 'InceptionV3/AuxLogits/Conv2d_1b_1x1/weights:0' shape=(1, 1, 768, 128) dtype=float32>, <tf.Variable 'InceptionV3/AuxLogits/Conv2d_1b_1x1/BatchNorm/beta:0' shape=(128,) dtype=float32>, <tf.Variable 'InceptionV3/AuxLogits/Conv2d_2a_5x5/weights:0' shape=(5, 5, 128, 768) dtype=float32>, <tf.Variable 'InceptionV3/AuxLogits/Conv2d_2a_5x5/BatchNorm/beta:0' shape=(768,) dtype=float32>, <tf.Variable 'InceptionV3/AuxLogits/Conv2d_2b_1x1/weights:0' shape=(1, 1, 768, 28) dtype=float32>, <tf.Variable 'InceptionV3/AuxLogits/Conv2d_2b_1x1/biases:0' shape=(28,) dtype=float32>, <tf.Variable 'InceptionV3/Logits/Conv2d_1c_1x1/weights:0' shape=(1, 1, 128, 28) dtype=float32>, <tf.Variable 'InceptionV3/Logits/Conv2d_1c_1x1/biases:0' shape=(28,) dtype=float32>]\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt', vars_to_warm_start=[], var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
            "INFO:tensorflow:Warm-starting from: ('gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt',)\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 8 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (48) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (48) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 9.010987, step = 48\n",
            "INFO:tensorflow:Saving checkpoints for 48 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 9.010987.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "INFO:tensorflow:Precision: float32\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.24.81.122:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f99c9c303c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.24.81.122:8470', '_evaluation_master': b'grpc://10.24.81.122:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f99c9c929b0>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Starting training ...\n",
            "training for \n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.24.81.122:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 17779721942798519834)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14974208131344447166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2051734382117691682)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8244368593160874389)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4711608759806958056)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3007811163809719381)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12529572917288567259)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16269313086056972315)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14915623603826003088)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 747416940851313804)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 229308682914320321)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5579812379159691080)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(128, 128), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(128, 1, 1, 128), dtype=float32)\n",
            "INFO:tensorflow:Using RMS optimizer\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt', vars_to_warm_start=[], var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
            "INFO:tensorflow:Warm-starting from: ('gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt',)\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-48\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 48 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 8 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 9.880525, step = 148\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 9.294946, step = 248 (88.825 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.1258\n",
            "INFO:tensorflow:examples/sec: 1152.82\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 7.83256, step = 348 (81.319 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.22973\n",
            "INFO:tensorflow:examples/sec: 1259.24\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 6.466939, step = 448 (81.395 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.22854\n",
            "INFO:tensorflow:examples/sec: 1258.02\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 6.912947, step = 548 (81.662 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.22456\n",
            "INFO:tensorflow:examples/sec: 1253.95\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 9.025439, step = 648 (82.723 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.20889\n",
            "INFO:tensorflow:examples/sec: 1237.91\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 6.305724, step = 748 (85.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.17298\n",
            "INFO:tensorflow:examples/sec: 1201.13\n",
            "INFO:tensorflow:Enqueue next (28) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (28) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 6.983516, step = 776 (21.390 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 776 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 6.983516.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "INFO:tensorflow:Precision: float32\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.24.81.122:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f99bb355f98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.24.81.122:8470', '_evaluation_master': b'grpc://10.24.81.122:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f99c5f78080>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Starting training ...\n",
            "training for \n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.24.81.122:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 17779721942798519834)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14974208131344447166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2051734382117691682)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8244368593160874389)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4711608759806958056)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3007811163809719381)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12529572917288567259)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16269313086056972315)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14915623603826003088)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 747416940851313804)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 229308682914320321)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5579812379159691080)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(128, 128), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(128, 1, 1, 128), dtype=float32)\n",
            "INFO:tensorflow:Using RMS optimizer\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt', vars_to_warm_start=[], var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
            "INFO:tensorflow:Warm-starting from: ('gs://human-protein-atlas-kaggle/pre-trained/inception_v3.ckpt',)\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-776\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 776 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 8 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 4.185673, step = 876\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 6.9638414, step = 976 (90.769 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.1017\n",
            "INFO:tensorflow:examples/sec: 1128.14\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 5.2791452, step = 1076 (80.496 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.2423\n",
            "INFO:tensorflow:examples/sec: 1272.12\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 8.1210575, step = 1176 (81.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.22053\n",
            "INFO:tensorflow:examples/sec: 1249.82\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 6.2924204, step = 1276 (81.032 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.23408\n",
            "INFO:tensorflow:examples/sec: 1263.7\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 4.4210715, step = 1376 (82.604 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.2106\n",
            "INFO:tensorflow:examples/sec: 1239.65\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 4.6734443, step = 1476 (85.158 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.17429\n",
            "INFO:tensorflow:examples/sec: 1202.48\n",
            "INFO:tensorflow:Enqueue next (28) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (28) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 5.0767965, step = 1504 (21.313 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1504 into gs://human-protein-atlas-kaggle/output/model.ckpt.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 5.0767965.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TYN3etXr2NRM",
        "colab_type": "code",
        "outputId": "321ee37d-0700-4943-9192-85bd30b0eebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1437
        }
      },
      "cell_type": "code",
      "source": [
        "do_it('eval')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Precision: float32\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.24.81.122:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f99b1a17780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.24.81.122:8470', '_evaluation_master': b'grpc://10.24.81.122:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=6, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f99b7a0bd68>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Waiting for new checkpoint at gs://human-protein-atlas-kaggle/output/\n",
            "INFO:tensorflow:Found new checkpoint at gs://human-protein-atlas-kaggle/output/model.ckpt-1504\n",
            "INFO:tensorflow:Starting to evaluate.\n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.24.81.122:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 17779721942798519834)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14974208131344447166)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2051734382117691682)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8244368593160874389)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4711608759806958056)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3007811163809719381)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12529572917288567259)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16269313086056972315)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14915623603826003088)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 747416940851313804)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 229308682914320321)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5579812379159691080)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(128, 128), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(128, 1, 1, 128), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-11-17-21:42:45\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-1504\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Reloading EMA...\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-1504\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (6) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (6) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Evaluation [6/6]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2018-11-17-21:43:56\n",
            "INFO:tensorflow:Saving dict for global step 1504: f1 = 0.32779154, global_step = 1504, loss = 4.048288, precision = 0.47963887, recall = 0.2488043\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1504: gs://human-protein-atlas-kaggle/output/model.ckpt-1504\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "INFO:tensorflow:Eval results: {'f1': 0.32779154, 'loss': 4.048288, 'precision': 0.47963887, 'recall': 0.2488043, 'global_step': 1504}. Elapsed seconds: 82\n",
            "INFO:tensorflow:Evaluation finished after training step 1504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NgCutQhuc1B9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "do_it('predict')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KveM9YMbKeWi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}