{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPUInceptionV3_RNN_TF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "najlB4tnapzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install imgaug\n",
        "#!pip install --upgrade tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4wTFMNrAZY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/competitions/human-protein-atlas-image-classification/output\n",
        "import os\n",
        "os.chdir('/content/competitions/human-protein-atlas-image-classification')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RVBQEkhE1wFO",
        "colab_type": "code",
        "outputId": "a69cbb57-3aa0-419e-82e8-d3e07a6a0b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "# Upload credentials to TPU.\n",
        "with tf.Session() as session:\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "        tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "# Now credentials are set for all future sessions on this TPU.\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "    print('Not connected to a TPU runtime')\n",
        "    use_tpu=False\n",
        "else:\n",
        "    use_tpu=True\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "    with tf.Session(TPU_ADDRESS) as session:\n",
        "        print('TPU devices:')\n",
        "        pprint.pprint(session.list_devices())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not connected to a TPU runtime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a5OO_GRS5PdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "from skimage.transform import resize\n",
        "#from imgaug import augmenters as iaa\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SIZE = 299\n",
        "SEED = 777\n",
        "THRESHOLD = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TopFylv55Pde",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UAu2PQ_PD_fX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GCS access helpers ##\n",
        "Courtesy of https://stackoverflow.com/a/52106361/7724174\n",
        "\n",
        "These functions let us get data from GCS into our notebook."
      ]
    },
    {
      "metadata": {
        "id": "bDZlL-_K5Pdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load dataset info\n",
        "#DIR = '../input/'\n",
        "#DIR='gs://human-protein-atlas-kaggle/'\n",
        "#data = dd.read_csv(DIR+'train.csv')\n",
        "#data = data.compute()\n",
        "\n",
        "DATA_DIR='gs://human-protein-atlas-kaggle/'\n",
        "\n",
        "from tensorflow.python.lib.io import file_io\n",
        "with file_io.FileIO(DATA_DIR+'train.csv', 'r') as f:\n",
        "    data = pd.read_csv(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRMpE3B95Pdr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SHAPE = (299, 299, 3)\n",
        "NUM_CLASSES=28\n",
        "#epochs = 400;\n",
        "epochs = 200\n",
        "#batch_size = 256;\n",
        "VAL_RATIO = .1;\n",
        "DEBUG = False\n",
        "channels = [\"green\", \"blue\", \"red\"]\n",
        "lstmUnits=64\n",
        "DEBUG=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ciXtnu0dfPO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data input pipline ##\n",
        "This isn't fully optimized yet, but it's good enough."
      ]
    },
    {
      "metadata": {
        "id": "kAm-HbRHN-U3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TF_DIR=DATA_DIR+'train'\n",
        "DS_DIMS=[512,512]\n",
        "NN_DIMS=[299,299]\n",
        "REC_BUF_SIZE=453762 # This is approximate size for 512x512 images\n",
        "NUM_PARALLEL_CALLS=8 # number of cores in the system\n",
        "class HPADataset:\n",
        "    def __init__(self, shards, aug=True, input_path=TF_DIR, batch_size=128):\n",
        "        self.shards = shards\n",
        "        self.aug = aug\n",
        "        self.input_path = input_path\n",
        "        self.def_batch_size = batch_size\n",
        "    def input_fn(self, params):\n",
        "        if 'batch_size' in params:\n",
        "            batch_size=params['batch_size']\n",
        "        else:\n",
        "            batch_size=self.def_batch_size\n",
        "        def _parse_function(example_proto):\n",
        "            features = {}\n",
        "            for c in channels:\n",
        "                features[\"image/%s/filename\"%c] = tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "                features[\"image/%s/encoded\"%c] = tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "            features[\"image/label\"] = tf.FixedLenFeature((NUM_CLASSES), tf.float32, default_value=[0]*NUM_CLASSES)\n",
        "            features[\"image/id\"]=tf.FixedLenFeature((), tf.string, default_value='')\n",
        "            #features[\"image/id\"]=tf.FixedLenFeature((1024), tf.int8, default_value=0)\n",
        "            parsed_features = tf.parse_single_example(example_proto, features)\n",
        "            imgs=[]\n",
        "            for c in channels:\n",
        "                img=parsed_features['image/%s/encoded'%c]\n",
        "                #print(img)\n",
        "                img=tf.image.decode_png(img, channels=1)\n",
        "                shape=tf.shape(img)\n",
        "                #shape_print=tf.print(shape)\n",
        "                img=tf.reshape(img, DS_DIMS)\n",
        "                imgs.append(img)\n",
        "            image=tf.stack(imgs, axis=-1, name='combine_channels')\n",
        "            image=tf.image.resize_images(image, NN_DIMS)\n",
        "            # For simplicity, we'll use imgaug with py_op here\n",
        "            def augment(image):\n",
        "                augment_img = iaa.Sequential([\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Affine(rotate=0),\n",
        "                        iaa.Affine(rotate=90),\n",
        "                        iaa.Affine(rotate=180),\n",
        "                        iaa.Affine(rotate=270),\n",
        "                        iaa.Fliplr(0.5),\n",
        "                        iaa.Flipud(0.5),\n",
        "                    ])], random_order=True)\n",
        "\n",
        "                image_aug = augment_img.augment_image(image)\n",
        "                return image_aug\n",
        "            if self.aug:\n",
        "                image=tf.py_func(augment, [image], tf.float32, name='augment')\n",
        "                image=tf.reshape(image, NN_DIMS+[len(channels)])\n",
        "            image=tf.cast(image, tf.float32)\n",
        "            image=image / 255.\n",
        "            \n",
        "            ids=parsed_features['image/id']\n",
        "            return image, ids, parsed_features[\"image/label\"]\n",
        "\n",
        "        def parse_ids(image, ids, labels):\n",
        "            #print(\"ids\",ids)\n",
        "            ids=tf.strings.split(ids, '-')\n",
        "            #print(\"ids\",ids)\n",
        "            ids=tf.sparse.to_dense(ids, default_value='')\n",
        "            #print(\"ids\",ids)\n",
        "            ids=tf.strings.to_number(ids, tf.int32)\n",
        "            #print(\"ids\",ids)\n",
        "            ids=tf.reshape(ids, (-1, 5))\n",
        "            #print(\"ids\",ids)\n",
        "            return image, ids, labels\n",
        "\n",
        "        fnames=['{dir}/hpa_{w}x{h}_{num}.tfrecords'.format(dir=self.input_path, w=DS_DIMS[0], h=DS_DIMS[1], num=shard) for shard in self.shards]\n",
        "        dataset=tf.data.TFRecordDataset(fnames,\n",
        "                                        buffer_size=REC_BUF_SIZE*2*len(self.shards),\n",
        "                                        num_parallel_reads=len(self.shards))\n",
        "        dataset=dataset.map(_parse_function, num_parallel_calls=NUM_PARALLEL_CALLS)\n",
        "        if params['mode']!='predict':\n",
        "            dataset=dataset.shuffle(1000)\n",
        "        dataset=dataset.prefetch(batch_size*8)\n",
        "        dataset=dataset.batch(batch_size, drop_remainder=False)\n",
        "        #dataset=dataset.map(parse_ids, num_parallel_calls=NUM_PARALLEL_CALLS)\n",
        "        if params['mode']=='predict' and False:\n",
        "            def pad_batch(img, ids, labels):\n",
        "                def pd(x):\n",
        "                    paddings = [\n",
        "                        [0, batch_size-tf.shape(img)[0]],\n",
        "                    ]\n",
        "                    for i in range(1,len(x.shape)):\n",
        "                        paddings.append([0, 0])\n",
        "                    pad = tf.pad(x, paddings)\n",
        "                    shape = pad.shape.as_list()\n",
        "                    shape[0] = batch_size\n",
        "                    return tf.reshape(pad, shape)\n",
        "                return {'image':pd(img), 'ids':pd(ids)}, pd(labels)\n",
        "                #return pd(img), pd(ids), pd(labels)\n",
        "            dataset=dataset.map(pad_batch)\n",
        "        dataset=dataset.prefetch(2)\n",
        "        if params['mode']=='predict' and False:\n",
        "            #print(\"dataset!\", dataset)\n",
        "            return dataset\n",
        "        if params['mode']!='predict':\n",
        "            dataset=dataset.repeat()\n",
        "        img, ids, labels = dataset.make_one_shot_iterator().get_next()\n",
        "        ret={'image':img, 'ids':ids, 'ds_labels': labels}\n",
        "        #print(\"ret\", ret)\n",
        "        return ret, labels\n",
        "#with tf.Graph().as_default():\n",
        "#    test=HPADataset([1,2], False).input_fn({\n",
        "#        'mode':'predict',\n",
        "#        'batch_size': 1024\n",
        "#    })\n",
        "#    with tf.Session() as sess:\n",
        "#        sess.run(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_VNMbwOsvV-4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tg = HPADataset(range(8), False)\n",
        "vg = HPADataset([8, 9], False)\n",
        "test=HPADataset(range(10), False, input_path=DATA_DIR+'test', batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoxbV4Hsd1jA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model ##\n",
        "\n",
        "What follows is our model based on pure tensorflow and the inception model present there. Large portions of this code are lifted from https://github.com/tensorflow/tpu/blob/master/models/experimental/inception/inception_v3.py\n"
      ]
    },
    {
      "metadata": {
        "id": "kD7NGt2Y5Pdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import summary\n",
        "from tensorflow.contrib.framework.python.ops import arg_scope\n",
        "from tensorflow.contrib.slim.nets import inception\n",
        "from tensorflow.contrib.training.python.training import evaluation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k3QEuM7qf4rp",
        "colab_type": "code",
        "outputId": "426fcfb4-04ed-4383-f714-8228e8fec1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "### Some model settings\n",
        "precision='float32'\n",
        "log_device_placement=False\n",
        "clear_update_collections=True\n",
        "num_classes=NUM_CLASSES\n",
        "display_tensors=False\n",
        "#train_batch_size=1024\n",
        "#geval_batch_size=1024\n",
        "train_batch_size=128\n",
        "geval_batch_size=128\n",
        "glearning_rate=0.165\n",
        "learning_rate_decay=0.94\n",
        "use_learning_rate_warmup=False\n",
        "warmup_epochs=7\n",
        "cold_epochs=2\n",
        "learning_rate_decay_epochs=6\n",
        "skip_host_call=True\n",
        "goptimizer='RMS'\n",
        "moving_average=False\n",
        "MOVING_AVERAGE_DECAY = 0.995\n",
        "# Batchnorm moving mean/variance parameters\n",
        "BATCH_NORM_DECAY = 0.996\n",
        "BATCH_NORM_EPSILON = 1e-3\n",
        "\n",
        "WEIGHT_DECAY = 0.00004\n",
        "RMSPROP_DECAY = 0.9                # Decay term for RMSProp.\n",
        "RMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\n",
        "RMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n",
        "\n",
        "min_depth=16\n",
        "transpose_enabled=False\n",
        "spatial_squeeze=True\n",
        "\n",
        "_NUM_TRAIN_IMAGES = 24858\n",
        "_NUM_EVAL_IMAGES = 6214\n",
        "ITERATIONS=_NUM_TRAIN_IMAGES*epochs/train_batch_size\n",
        "num_shards=8 # 8 in original..\n",
        "#model_dir='/content/competitions/human-protein-atlas-image-classification/output'\n",
        "model_dir=DATA_DIR+'output/'\n",
        "save_checkpoints_secs=1000\n",
        "save_summary_steps=100\n",
        "eval_timeout=None\n",
        "train_steps_per_eval=int(_NUM_TRAIN_IMAGES/train_batch_size) # essentially one epoch\n",
        "\n",
        "dropout_keep_prob=0.8\n",
        "train_steps=int(ITERATIONS)\n",
        "print('Will train for {train} steps'.format(train=train_steps))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Will train for 38840 steps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9NYAVMEH45rM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def tensor_transform_fn(data, perm):\n",
        "  \"\"\"Transpose function.\n",
        "\n",
        "  This function is used to transpose an image tensor on the host and then\n",
        "  perform an inverse transpose on the TPU. The transpose on the TPU gets\n",
        "  effectively elided thus voiding any associated computational cost.\n",
        "\n",
        "  NOTE: Eventually the compiler will be able to detect when this kind of\n",
        "  operation may prove beneficial and perform these types of transformations\n",
        "  implicitly, voiding the need for user intervention\n",
        "\n",
        "  Args:\n",
        "    data: Tensor to be transposed\n",
        "    perm: New ordering of dimensions\n",
        "\n",
        "  Returns:\n",
        "    Transposed tensor\n",
        "  \"\"\"\n",
        "  if transpose_enabled:\n",
        "    return tf.transpose(data, perm)\n",
        "  return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Up9dhjFpe-14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib import layers\n",
        "from tensorflow.contrib.layers.python.layers import layers as layers_lib\n",
        "from tensorflow.python.ops import array_ops\n",
        "\n",
        "def inception_model_fn(features, labels, mode, params):\n",
        "    \"\"\"Inception v3 model using Estimator API.\"\"\"\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n",
        "\n",
        "    if DEBUG:\n",
        "        print(\"Features\", features)\n",
        "        print(\"Labels\", labels)\n",
        "        print(\"Mode\", mode)\n",
        "        print(\"Params\", params)\n",
        "    if isinstance(features, dict):\n",
        "        ids = features['ids']\n",
        "        ds_labels = features['ds_labels']\n",
        "        features = features['image']\n",
        "    else:\n",
        "        print(\"Features should be a dictionary with 'image' and 'id' keys\")\n",
        "        raise ValueError('invalid input')\n",
        "\n",
        "    features = tensor_transform_fn(features, params['input_perm'])\n",
        "\n",
        "    # This nested function allows us to avoid duplicating the logic which\n",
        "    # builds the network, for different values of --precision.\n",
        "    def build_inception_v3(final_endpoint='Mixed_7c', scope=None):\n",
        "        with tf.variable_scope(\n",
        "            scope, 'InceptionV3', [features, num_classes], reuse=None) as scope:\n",
        "            with arg_scope(\n",
        "              [layers_lib.batch_norm, layers_lib.dropout], is_training=is_training):\n",
        "              net, end_points = inception.inception_v3_base(\n",
        "                  features,\n",
        "                  final_endpoint=final_endpoint\n",
        "              )\n",
        "\n",
        "              # Build our RNNs\n",
        "              def build_rnn(endpoint):\n",
        "                # 1. Unroll our logits horisonatlly and vertically\n",
        "                #print(\"Input endpoint\", endpoint)\n",
        "                shape=endpoint.shape.as_list()\n",
        "                if shape[0] is None:\n",
        "                    bd=-1\n",
        "                else:\n",
        "                    bd=shape[0]\n",
        "                shape=(bd, shape[1]*shape[2], shape[3])\n",
        "                input_width=tf.reshape(endpoint, shape)\n",
        "                input_height=tf.transpose(endpoint, (0, 2, 1, 3))\n",
        "                input_height=tf.reshape(input_height, shape)\n",
        "                lstmCellW = tf.contrib.rnn.LSTMCell(num_units=lstmUnits, use_peepholes=True)\n",
        "                lstmCellH = tf.contrib.rnn.LSTMCell(num_units=lstmUnits, use_peepholes=True)\n",
        "                wouts, wstate=tf.nn.dynamic_rnn(lstmCellW, input_width, scope='width', dtype=tf.float32, parallel_iterations=1024)\n",
        "                houts, hstate=tf.nn.dynamic_rnn(lstmCellH, input_height, scope='height', dtype=tf.float32, parallel_iterations=1024)\n",
        "                logits=tf.concat([wstate.h, hstate.h], name='Logits', axis=-1)\n",
        "                return logits\n",
        "              rnn_logits=[]\n",
        "              attachements=['Mixed_7b', 'Mixed_7a', 'Mixed_6e']\n",
        "              #attachements=['Mixed_7a']\n",
        "              for att_pt in attachements:\n",
        "                    with tf.variable_scope(att_pt+'_rnn'):\n",
        "                        rnn_logits.append(build_rnn(end_points[att_pt]))\n",
        "              rnn_logits=tf.concat(rnn_logits, name='FinalRNNLogits', axis=-1)\n",
        "\n",
        "              depth = lambda d: max(d, min_depth)\n",
        "              if 'Mixed_6e' in end_points:\n",
        "                  # Auxiliary Head logits\n",
        "                  with arg_scope(\n",
        "                      [layers.conv2d, layers_lib.max_pool2d, layers_lib.avg_pool2d],\n",
        "                      stride=1,\n",
        "                      padding='SAME'):\n",
        "                    aux_logits = end_points['Mixed_6e']\n",
        "                    with tf.variable_scope('AuxLogits'):\n",
        "                      aux_logits = layers_lib.avg_pool2d(\n",
        "                          aux_logits, [5, 5],\n",
        "                          stride=3,\n",
        "                          padding='VALID',\n",
        "                          scope='AvgPool_1a_5x5')\n",
        "                      aux_logits = layers.conv2d(\n",
        "                          aux_logits, depth(128), [1, 1], scope='Conv2d_1b_1x1')\n",
        "\n",
        "                      # Shape of feature map before the final layer.\n",
        "                      kernel_size = [5, 5]\n",
        "                      aux_logits = layers.conv2d(\n",
        "                          aux_logits,\n",
        "                          depth(768),\n",
        "                          kernel_size,\n",
        "                          weights_initializer=tf.initializers.truncated_normal(stddev=0.01),\n",
        "                          padding='VALID',\n",
        "                          scope='Conv2d_2a_{}x{}'.format(*kernel_size))\n",
        "                      aux_logits = layers.conv2d(\n",
        "                          aux_logits,\n",
        "                          num_classes, [1, 1],\n",
        "                          activation_fn=None,\n",
        "                          normalizer_fn=None,\n",
        "                          weights_initializer=tf.initializers.truncated_normal(stddev=0.001),\n",
        "                          scope='Conv2d_2b_1x1')\n",
        "                      if spatial_squeeze:\n",
        "                        aux_logits = array_ops.squeeze(\n",
        "                            aux_logits, name='SpatialSqueeze')\n",
        "                      end_points['AuxLogits'] = aux_logits\n",
        "            # Final pooling and prediction\n",
        "            with tf.variable_scope('Logits'):\n",
        "                #kernel_size = [8,8]\n",
        "                #net = layers_lib.avg_pool2d(net, kernel_size, padding='VALID', scope='AvgPool_1a{}x{}'.format(*kernel_size))\n",
        "                ## 1x1x2048\n",
        "                #net = layers_lib.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
        "                #end_points['PreLogits']=net\n",
        "                ##2048\n",
        "                #logits = layers.conv2d(net, num_classes, [1,1], activation_fn=None, normalizer_fn=None, scope='Conv2d_1c_1x1')\n",
        "                #if spatial_squeeze:\n",
        "                #    logits = array_ops.squeeze(logits, [1,2], name='SpatialSqueeze')\n",
        "                #end_points['Logits']=logits\n",
        "                #end_points['Predictions']=tf.nn.sigmoid(logits, name='Predicitons')\n",
        "                #return logits, end_points\n",
        "\n",
        "                net = rnn_logits\n",
        "                end_points['PreLogits']=net\n",
        "                end_points['rnn_logits']=net\n",
        "                net = layers_lib.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
        "                print(net)\n",
        "                net = tf.expand_dims(net, 1)\n",
        "                net = tf.expand_dims(net, 1)\n",
        "                print(net)\n",
        "                logits = layers.conv2d(net, num_classes, 1, activation_fn=None, normalizer_fn=None, scope='Conv2d_1c_1x1')\n",
        "                if spatial_squeeze:\n",
        "                    logits = array_ops.squeeze(logits, [1,2], name='SpatialSqueeze')\n",
        "                # 28\n",
        "                end_points['Logits']=logits\n",
        "                end_points['Predictions'] = tf.nn.sigmoid(logits, name='Predictions')\n",
        "                return logits, end_points\n",
        "\n",
        "    def build_network(precision):\n",
        "        if precision == 'bfloat16':\n",
        "            with tf.contrib.tpu.bfloat16_scope():\n",
        "                logits, end_points = build_inception_v3()\n",
        "            logits = tf.cast(logits, tf.float32)\n",
        "        elif precision == 'float32':\n",
        "            logits, end_points = build_inception_v3()\n",
        "        return logits, end_points\n",
        "\n",
        "    if clear_update_collections:\n",
        "        # updates_collections must be set to None in order to use fused batchnorm\n",
        "        with arg_scope(inception.inception_v3_arg_scope(\n",
        "            weight_decay=0.0,\n",
        "            batch_norm_decay=BATCH_NORM_DECAY,\n",
        "            batch_norm_epsilon=BATCH_NORM_EPSILON,\n",
        "            updates_collections=None)):\n",
        "            logits, end_points = build_network('float32')\n",
        "    else:\n",
        "        with arg_scope(inception.inception_v3_arg_scope(\n",
        "            batch_norm_decay=BATCH_NORM_DECAY,\n",
        "            batch_norm_epsilon=BATCH_NORM_EPSILON)):\n",
        "            logits, end_points = build_network('float32')\n",
        "\n",
        "    predictions = {\n",
        "        'logits': logits,\n",
        "        'classes': tf.math.greater(logits, 0.2),\n",
        "        'probabilities': end_points['Predictions'],\n",
        "        'predictions': end_points['Predictions'],\n",
        "        'ids': ids,\n",
        "        'ds_labels': ds_labels,\n",
        "        #'rnn_logits': end_points['rnn_logits'],\n",
        "        #'dropout': end_points['dropout'],\n",
        "    }\n",
        "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
        "        predictions['labels'] = labels\n",
        "    #print(\"predictions\", predictions)\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            predictions=predictions,\n",
        "            export_outputs={\n",
        "                'classify': tf.estimator.export.PredictOutput(predictions)\n",
        "            })\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.EVAL and display_tensors and (\n",
        "        not use_tpu):\n",
        "        with tf.control_dependencies([\n",
        "            tf.Print(\n",
        "                predictions['classes'], [predictions['classes']],\n",
        "                summarize=geval_batch_size,\n",
        "                message='prediction: ')\n",
        "        ]):\n",
        "            labels = tf.Print(\n",
        "                labels, [labels], summarize=geval_batch_size, message='label: ')\n",
        "\n",
        "    # in our case labels come pre-encoded\n",
        "    one_hot_labels = labels #tf.one_hot(labels, num_classes, dtype=tf.int32)\n",
        "\n",
        "    if 'AuxLogits' in end_points:\n",
        "        tf.losses.sigmoid_cross_entropy(\n",
        "            multi_class_labels=one_hot_labels,\n",
        "            logits=tf.cast(end_points['AuxLogits'], tf.float32),\n",
        "            weights=0.4,\n",
        "            label_smoothing=0.1,\n",
        "            scope='aux_loss')\n",
        "\n",
        "    tf.losses.sigmoid_cross_entropy(\n",
        "        multi_class_labels=one_hot_labels,\n",
        "        logits=logits,\n",
        "        #weights=1.0,\n",
        "        #label_smoothing=0.1,\n",
        "        reduction=tf.losses.Reduction.MEAN\n",
        "    )\n",
        "\n",
        "    losses = tf.add_n(tf.losses.get_losses())\n",
        "    l2_loss = []\n",
        "    for v in tf.trainable_variables():\n",
        "        if 'BatchNorm' not in v.name and 'weights' in v.name:\n",
        "            l2_loss.append(tf.nn.l2_loss(v))\n",
        "    loss = losses + WEIGHT_DECAY * tf.add_n(l2_loss)\n",
        "\n",
        "    initial_learning_rate = glearning_rate * train_batch_size / 256\n",
        "    if use_learning_rate_warmup:\n",
        "        # Adjust initial learning rate to match final warmup rate\n",
        "        warmup_decay = learning_rate_decay**(\n",
        "            (warmup_epochs + cold_epochs) /\n",
        "            learning_rate_decay_epochs)\n",
        "        adj_initial_learning_rate = initial_learning_rate * warmup_decay\n",
        "\n",
        "    final_learning_rate = 0.0001 * initial_learning_rate\n",
        "\n",
        "    host_call = None\n",
        "    train_op = None\n",
        "  \n",
        "    if is_training:\n",
        "        batches_per_epoch = _NUM_TRAIN_IMAGES / train_batch_size\n",
        "        global_step = tf.train.get_or_create_global_step()\n",
        "        current_epoch = tf.cast(\n",
        "            (tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\n",
        "\n",
        "        learning_rate = tf.train.exponential_decay(\n",
        "            learning_rate=initial_learning_rate,\n",
        "            global_step=global_step,\n",
        "            decay_steps=int(learning_rate_decay_epochs * batches_per_epoch),\n",
        "            decay_rate=learning_rate_decay,\n",
        "            staircase=True)\n",
        "\n",
        "        if use_learning_rate_warmup:\n",
        "            wlr = 0.1 * adj_initial_learning_rate\n",
        "            wlr_height = tf.cast(\n",
        "                0.9 * adj_initial_learning_rate /\n",
        "                (warmup_epochs + learning_rate_decay_epochs - 1),\n",
        "                tf.float32)\n",
        "            epoch_offset = tf.cast(cold_epochs - 1, tf.int32)\n",
        "            exp_decay_start = (warmup_epochs + cold_epochs +\n",
        "                             learning_rate_decay_epochs)\n",
        "            lin_inc_lr = tf.add(\n",
        "                wlr, tf.multiply(\n",
        "                    tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32),\n",
        "                    wlr_height))\n",
        "            learning_rate = tf.where(\n",
        "                tf.greater_equal(current_epoch, cold_epochs),\n",
        "                (tf.where(tf.greater_equal(current_epoch, exp_decay_start),\n",
        "                          learning_rate, lin_inc_lr)),\n",
        "                wlr)\n",
        "\n",
        "        # Set a minimum boundary for the learning rate.\n",
        "        learning_rate = tf.maximum(\n",
        "            learning_rate, final_learning_rate, name='learning_rate')\n",
        "\n",
        "        if goptimizer == 'sgd':\n",
        "            tf.logging.info('Using SGD optimizer')\n",
        "            optimizer = tf.train.GradientDescentOptimizer(\n",
        "                learning_rate=learning_rate)\n",
        "        elif goptimizer == 'momentum':\n",
        "            tf.logging.info('Using Momentum optimizer')\n",
        "            optimizer = tf.train.MomentumOptimizer(\n",
        "                learning_rate=learning_rate, momentum=0.9)\n",
        "        elif goptimizer == 'RMS':\n",
        "            tf.logging.info('Using RMS optimizer')\n",
        "            optimizer = tf.train.RMSPropOptimizer(\n",
        "                learning_rate,\n",
        "                RMSPROP_DECAY,\n",
        "                momentum=RMSPROP_MOMENTUM,\n",
        "                epsilon=RMSPROP_EPSILON)\n",
        "        else:\n",
        "            tf.logging.fatal('Unknown optimizer:', optimizer)\n",
        "\n",
        "        if use_tpu:\n",
        "            optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        if params['warmup']:\n",
        "            trainable_vars = tf.contrib.framework.get_model_variables()\n",
        "            var_list=tf.contrib.framework.filter_variables(trainable_vars, exclude_patterns='.*Mixed_.*_rnn.*')\n",
        "            trainable_vars = tf.contrib.framework.get_trainable_variables()\n",
        "            var_list=tf.contrib.framework.filter_variables(trainable_vars, exclude_patterns=var_list)\n",
        "            #print('Training', var_list)\n",
        "        else:\n",
        "            var_list=None\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            train_op = optimizer.minimize(loss, global_step=global_step, var_list=var_list)\n",
        "        if moving_average:\n",
        "            ema = tf.train.ExponentialMovingAverage(\n",
        "                decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\n",
        "            variables_to_average = (\n",
        "                tf.trainable_variables() + tf.moving_average_variables())\n",
        "            with tf.control_dependencies([train_op]), tf.name_scope('moving_average'):\n",
        "                train_op = ema.apply(variables_to_average)\n",
        "\n",
        "        # To log the loss, current learning rate, and epoch for Tensorboard, the\n",
        "        # summary op needs to be run on the host CPU via host_call. host_call\n",
        "        # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n",
        "        # dimension. These Tensors are implicitly concatenated to\n",
        "        # [params['batch_size']].\n",
        "        gs_t = tf.reshape(global_step, [1])\n",
        "        loss_t = tf.reshape(loss, [1])\n",
        "        lr_t = tf.reshape(learning_rate, [1])\n",
        "        ce_t = tf.reshape(current_epoch, [1])\n",
        "\n",
        "        if not skip_host_call:\n",
        "            def host_call_fn(gs, loss, lr, ce):\n",
        "                \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "                This function is executed on the CPU and should not directly reference\n",
        "                any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "                model to the `metric_fn`, provide them as part of the `host_call`. See\n",
        "                https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "                for more information.\n",
        "                Arguments should match the list of `Tensor` objects passed as the second\n",
        "                element in the tuple passed to `host_call`.\n",
        "                Args:\n",
        "                  gs: `Tensor with shape `[batch]` for the global_step\n",
        "                  loss: `Tensor` with shape `[batch]` for the training loss.\n",
        "                  lr: `Tensor` with shape `[batch]` for the learning_rate.\n",
        "                  ce: `Tensor` with shape `[batch]` for the current_epoch.\n",
        "                Returns:\n",
        "                  List of summary ops to run on the CPU host.\n",
        "                \"\"\"\n",
        "                gs = gs[0]\n",
        "                with summary.create_file_writer(model_dir).as_default():\n",
        "                    with summary.always_record_summaries():\n",
        "                        summary.scalar('loss', tf.reduce_mean(loss), step=gs)\n",
        "                        summary.scalar('learning_rate', tf.reduce_mean(lr), step=gs)\n",
        "                        summary.scalar('current_epoch', tf.reduce_mean(ce), step=gs)\n",
        "\n",
        "                    return summary.all_summary_ops()\n",
        "\n",
        "            host_call = (host_call_fn, [gs_t, loss_t, lr_t, ce_t])\n",
        "\n",
        "    eval_metrics = None\n",
        "    if is_eval:\n",
        "        def metric_fn(labels, logits):\n",
        "            \"\"\"Evaluation metric function. Evaluates accuracy.\n",
        "            This function is executed on the CPU and should not directly reference\n",
        "            any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n",
        "            to the `metric_fn`, provide as part of the `eval_metrics`. See\n",
        "            https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "            for more information.\n",
        "            Arguments should match the list of `Tensor` objects passed as the second\n",
        "            element in the tuple passed to `eval_metrics`.\n",
        "            Args:\n",
        "            labels: `Tensor` with shape `[batch, ]`.\n",
        "            logits: `Tensor` with shape `[batch, num_classes]`.\n",
        "            Returns:\n",
        "            A dict of the metrics to return from evaluation.\n",
        "            \"\"\"\n",
        "            probs=tf.nn.sigmoid(logits)\n",
        "            predictions = tf.math.greater(probs, 0.2)\n",
        "            recall = tf.metrics.recall(labels, predictions)\n",
        "            precision=tf.metrics.precision(labels, predictions)\n",
        "            f1=tf.contrib.metrics.f1_score(labels, probs)\n",
        "\n",
        "            return {\n",
        "              'recall': recall,\n",
        "              'precision': precision,\n",
        "              'f1': f1\n",
        "            }\n",
        "\n",
        "        eval_metrics = (metric_fn, [labels, logits])\n",
        "\n",
        "    if use_tpu:\n",
        "        return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "            mode=mode,\n",
        "            loss=loss,\n",
        "            train_op=train_op,\n",
        "            host_call=host_call,\n",
        "            eval_metrics=eval_metrics)\n",
        "    else:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode=mode,\n",
        "            loss=loss,\n",
        "            train_op=train_op,\n",
        "            eval_metric_ops=metric_fn(labels, logits)\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1JcuU9F8VKkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4fSZX5_Tv44y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LoadEMAHook(tf.train.SessionRunHook):\n",
        "  \"\"\"Hook to load exponential moving averages into corresponding variables.\"\"\"\n",
        "\n",
        "  def __init__(self, model_dir):\n",
        "    super(LoadEMAHook, self).__init__()\n",
        "    self._model_dir = model_dir\n",
        "\n",
        "  def begin(self):\n",
        "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
        "    variables_to_restore = ema.variables_to_restore()\n",
        "    self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\n",
        "        tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\n",
        "\n",
        "  def after_create_session(self, sess, coord):\n",
        "    tf.logging.info('Reloading EMA...')\n",
        "    self._load_ema(sess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "atKLBN6BwJaZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def do_it(mode, predict_input_fn=test.input_fn):\n",
        "  params = {\n",
        "      'input_perm': [0, 1, 2, 3],\n",
        "      'output_perm': [0, 1, 2, 3],\n",
        "      'warmup': mode=='warmup',\n",
        "      'mode': mode,\n",
        "  }\n",
        "\n",
        "  if mode == 'retrain':\n",
        "    # wipe checkpoints\n",
        "    files=tf.gfile.ListDirectory(model_dir)\n",
        "    for f in files:\n",
        "        fname=model_dir+f\n",
        "        fs = tf.gfile.Stat(fname)\n",
        "        if not fs.is_directory:\n",
        "            tf.gfile.Remove(fname)\n",
        "\n",
        "    do_it('warmup')\n",
        "    do_it('train')\n",
        "  if mode == 'warmup':\n",
        "    mode = 'train'\n",
        "  if mode == 'retrain_and_eval':\n",
        "    # wipe checkpoints\n",
        "    files=tf.gfile.ListDirectory(model_dir)\n",
        "    for f in files:\n",
        "        fname=model_dir+f\n",
        "        fs = tf.gfile.Stat(fname)\n",
        "        if not fs.is_directory:\n",
        "            tf.gfile.Remove(fname)\n",
        "    do_it('warmup')\n",
        "    do_it('train_and_eval')\n",
        "\n",
        "  tf.logging.info('Precision: %s', precision)\n",
        "\n",
        "  if mode == 'predict':\n",
        "        batch_axis=None\n",
        "  else:\n",
        "    batch_axis = 0\n",
        "    if transpose_enabled:\n",
        "        params['input_perm'] = [3, 0, 1, 2]\n",
        "        params['output_perm'] = [1, 2, 3, 0]\n",
        "        batch_axis = 3\n",
        "    batch_axis=(batch_axis, 0)\n",
        "\n",
        "  eval_size = _NUM_EVAL_IMAGES\n",
        "  eval_steps = eval_size // geval_batch_size\n",
        "\n",
        "  iterations = (eval_steps if mode == 'eval' or mode == 'eval_all' else save_summary_steps)\n",
        "\n",
        "  eval_batch_size = (None if mode == 'train' else geval_batch_size)\n",
        "\n",
        "  per_host_input_for_training = (num_shards <= 8 if mode == 'train' else True)\n",
        "\n",
        "  if mode == 'predict' or not use_tpu:\n",
        "        run_config=tf.estimator.RunConfig(\n",
        "            model_dir=model_dir,\n",
        "            session_config=tf.ConfigProto(\n",
        "                allow_soft_placement=True,\n",
        "                log_device_placement=log_device_placement),\n",
        "        )\n",
        "  else:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "    run_config = tf.contrib.tpu.RunConfig(\n",
        "        cluster=tpu_cluster_resolver,\n",
        "        model_dir=model_dir,\n",
        "        save_checkpoints_secs=save_checkpoints_secs,\n",
        "        save_summary_steps=save_summary_steps,\n",
        "        keep_checkpoint_max=100,\n",
        "        session_config=tf.ConfigProto(\n",
        "            allow_soft_placement=True,\n",
        "            log_device_placement=log_device_placement),\n",
        "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "            iterations_per_loop=iterations,\n",
        "            num_shards=num_shards,\n",
        "            per_host_input_for_training=per_host_input_for_training))\n",
        "\n",
        "  trainable_vars = tf.contrib.framework.get_model_variables()\n",
        "  #print(trainable_vars)\n",
        "  skip_vars=['InceptionV3/AuxLogits/Conv2d_2b_1x1/weights']\n",
        "  load_vars = tf.contrib.framework.filter_variables(trainable_vars, exclude_patterns=skip_vars)\n",
        "  #print(load_vars)\n",
        "  ws = tf.estimator.WarmStartSettings(\n",
        "      ckpt_to_initialize_from=DATA_DIR+\"pre-trained/inception_v3.ckpt\",\n",
        "      vars_to_warm_start=load_vars\n",
        "  )\n",
        "\n",
        "  if mode == 'predict' or not use_tpu:\n",
        "    inception_classifier = tf.estimator.Estimator(\n",
        "        model_fn=inception_model_fn,\n",
        "        config=run_config,\n",
        "        warm_start_from=ws,\n",
        "        params=params\n",
        "    )\n",
        "  else:\n",
        "    inception_classifier = tf.contrib.tpu.TPUEstimator(\n",
        "        model_fn=inception_model_fn,\n",
        "        use_tpu=use_tpu,\n",
        "        config=run_config,\n",
        "        warm_start_from=ws,\n",
        "        params=params,\n",
        "        train_batch_size=train_batch_size,\n",
        "        eval_batch_size=eval_batch_size,\n",
        "        predict_batch_size=32,\n",
        "        batch_axis=batch_axis)\n",
        "    \n",
        "\n",
        "  # Input pipelines are slightly different (with regards to shuffling and\n",
        "  # preprocessing) between training and evaluation.\n",
        "  use_bfloat16 = precision == 'bfloat16'\n",
        "  imagenet_train = tg\n",
        "  imagenet_eval = vg\n",
        "\n",
        "  imagenet_predict=test\n",
        "\n",
        "  if moving_average:\n",
        "    eval_hooks = [LoadEMAHook(model_dir)]\n",
        "  else:\n",
        "    eval_hooks = []\n",
        "\n",
        "  if mode == 'eval':\n",
        "    # Run evaluation when there is a new checkpoint\n",
        "    for checkpoint in evaluation.checkpoints_iterator(\n",
        "        model_dir, timeout=eval_timeout):\n",
        "      tf.logging.info('Starting to evaluate.')\n",
        "      try:\n",
        "        start_timestamp = time.time()  # Includes compilation time\n",
        "        eval_results = inception_classifier.evaluate(\n",
        "            input_fn=imagenet_eval.input_fn,\n",
        "            steps=eval_steps,\n",
        "            hooks=eval_hooks,\n",
        "            checkpoint_path=checkpoint)\n",
        "        elapsed_time = int(time.time() - start_timestamp)\n",
        "        tf.logging.info(\n",
        "            'Eval results: %s. Elapsed seconds: %d', eval_results, elapsed_time)\n",
        "\n",
        "        # Terminate eval job when final checkpoint is reached\n",
        "        current_step = int(os.path.basename(checkpoint).split('-')[1])\n",
        "        if current_step >= train_steps:\n",
        "          tf.logging.info(\n",
        "              'Evaluation finished after training step %d', current_step)\n",
        "          break\n",
        "      except tf.errors.NotFoundError:\n",
        "        # Since the coordinator is on a different job than the TPU worker,\n",
        "        # sometimes the TPU worker does not finish initializing until long after\n",
        "        # the CPU job tells it to start evaluating. In this case, the checkpoint\n",
        "        # file could have been deleted already.\n",
        "        tf.logging.info(\n",
        "            'Checkpoint %s no longer exists, skipping checkpoint', checkpoint)\n",
        "  elif mode == 'eval_all':\n",
        "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
        "    for model_path in ckpt.all_model_checkpoint_paths:\n",
        "      print('Starting to evaluate ', model_path)\n",
        "      try:\n",
        "        start_timestamp = time.time()  # Includes compilation time\n",
        "        eval_results = inception_classifier.evaluate(\n",
        "            input_fn=imagenet_eval.input_fn,\n",
        "            steps=eval_steps,\n",
        "            hooks=eval_hooks,\n",
        "            checkpoint_path=model_path)\n",
        "        elapsed_time = int(time.time() - start_timestamp)\n",
        "        tf.logging.info(\n",
        "            'Eval results: %s. Elapsed seconds: %d', eval_results, elapsed_time)\n",
        "\n",
        "        # Terminate eval job when final checkpoint is reached\n",
        "        current_step = int(os.path.basename(model_path).split('-')[1])\n",
        "        if current_step >= train_steps:\n",
        "          tf.logging.info(\n",
        "              'Evaluation finished for training step %d', current_step)\n",
        "      except tf.errors.NotFoundError:\n",
        "        # Since the coordinator is on a different job than the TPU worker,\n",
        "        # sometimes the TPU worker does not finish initializing until long after\n",
        "        # the CPU job tells it to start evaluating. In this case, the checkpoint\n",
        "        # file could have been deleted already.\n",
        "        tf.logging.info(\n",
        "            'Checkpoint %s no longer exists, skipping checkpoint', model_path)\n",
        "        raise\n",
        "  elif mode == 'train_and_eval':\n",
        "    for cycle in range(train_steps // train_steps_per_eval):\n",
        "      tf.logging.info('Starting training cycle %d.' % cycle)\n",
        "      inception_classifier.train(\n",
        "          input_fn=imagenet_train.input_fn, steps=train_steps_per_eval)\n",
        "\n",
        "      tf.logging.info('Starting evaluation cycle %d .' % cycle)\n",
        "      eval_results = inception_classifier.evaluate(\n",
        "          input_fn=imagenet_eval.input_fn, steps=eval_steps, hooks=eval_hooks)\n",
        "      tf.logging.info('Evaluation results: %s' % eval_results)\n",
        "  elif mode == 'predict':\n",
        "    return inception_classifier.predict(input_fn=predict_input_fn)\n",
        "  else:\n",
        "    tf.logging.info('Starting training ...')\n",
        "    if params['warmup']:\n",
        "        steps = train_steps_per_eval*2 # ~2 epochs\n",
        "        print('warming up for ', steps)\n",
        "    else:\n",
        "        steps = train_steps\n",
        "        print('training for ')\n",
        "    inception_classifier.train(\n",
        "        input_fn=imagenet_train.input_fn, steps=steps)\n",
        "\n",
        "  #if export_dir is not None:\n",
        "  #  tf.logging.info('Starting to export model.')\n",
        "  #  inception_classifier.export_saved_model(\n",
        "  #      export_dir_base=export_dir,\n",
        "  #      serving_input_receiver_fn=image_serving_input_fn)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrRGqcnH5Pem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "#tf.logging.set_verbosity(tf.logging.INFO)\n",
        "#do_it('retrain')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYN3etXr2NRM",
        "colab_type": "code",
        "outputId": "c295c679-00ae-4be4-ba24-f68ebe70c0fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3693
        }
      },
      "cell_type": "code",
      "source": [
        "print(use_tpu)\n",
        "#do_it('eval')\n",
        "#do_it('eval_all')\n",
        "#error"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "INFO:tensorflow:Precision: float32\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f11b9a0b710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "Starting to evaluate  gs://human-protein-atlas-kaggle/output/model.ckpt-19808\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(?, 384), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(?, 1, 1, 384), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-12-01-21:34:26\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-19808\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [4/48]\n",
            "INFO:tensorflow:Evaluation [8/48]\n",
            "INFO:tensorflow:Evaluation [12/48]\n",
            "INFO:tensorflow:Evaluation [16/48]\n",
            "INFO:tensorflow:Evaluation [20/48]\n",
            "INFO:tensorflow:Evaluation [24/48]\n",
            "INFO:tensorflow:Evaluation [28/48]\n",
            "INFO:tensorflow:Evaluation [32/48]\n",
            "INFO:tensorflow:Evaluation [36/48]\n",
            "INFO:tensorflow:Evaluation [40/48]\n",
            "INFO:tensorflow:Evaluation [44/48]\n",
            "INFO:tensorflow:Evaluation [48/48]\n",
            "INFO:tensorflow:Finished evaluation at 2018-12-01-21:37:17\n",
            "INFO:tensorflow:Saving dict for global step 19808: f1 = 0.35799497, global_step = 19808, loss = 0.30709657, precision = 0.29583526, recall = 0.4451086\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 19808: gs://human-protein-atlas-kaggle/output/model.ckpt-19808\n",
            "INFO:tensorflow:Eval results: {'f1': 0.35799497, 'loss': 0.30709657, 'precision': 0.29583526, 'recall': 0.4451086, 'global_step': 19808}. Elapsed seconds: 180\n",
            "Starting to evaluate  gs://human-protein-atlas-kaggle/output/model.ckpt-24608\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(?, 384), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(?, 1, 1, 384), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-12-01-21:37:26\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-24608\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [4/48]\n",
            "INFO:tensorflow:Evaluation [8/48]\n",
            "INFO:tensorflow:Evaluation [12/48]\n",
            "INFO:tensorflow:Evaluation [16/48]\n",
            "INFO:tensorflow:Evaluation [20/48]\n",
            "INFO:tensorflow:Evaluation [24/48]\n",
            "INFO:tensorflow:Evaluation [28/48]\n",
            "INFO:tensorflow:Evaluation [32/48]\n",
            "INFO:tensorflow:Evaluation [36/48]\n",
            "INFO:tensorflow:Evaluation [40/48]\n",
            "INFO:tensorflow:Evaluation [44/48]\n",
            "INFO:tensorflow:Evaluation [48/48]\n",
            "INFO:tensorflow:Finished evaluation at 2018-12-01-21:40:03\n",
            "INFO:tensorflow:Saving dict for global step 24608: f1 = 0.3325286, global_step = 24608, loss = 0.30642107, precision = 0.2724118, recall = 0.42216912\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 24608: gs://human-protein-atlas-kaggle/output/model.ckpt-24608\n",
            "INFO:tensorflow:Eval results: {'f1': 0.3325286, 'loss': 0.30642107, 'precision': 0.2724118, 'recall': 0.42216912, 'global_step': 24608}. Elapsed seconds: 164\n",
            "Starting to evaluate  gs://human-protein-atlas-kaggle/output/model.ckpt-30008\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(?, 384), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(?, 1, 1, 384), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-12-01-21:40:10\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-30008\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-e6c4040b7c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#do_it('eval')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdo_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval_all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-a23fd922c077>\u001b[0m in \u001b[0;36mdo_it\u001b[0;34m(mode, predict_input_fn)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             checkpoint_path=model_path)\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_timestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         tf.logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0meval_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mall_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             output_dir=self.eval_dir(name))\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_run\u001b[0;34m(self, checkpoint_path, scaffold, update_op, eval_dict, all_hooks, output_dir)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         \u001b[0mfinal_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m         config=self._session_config)\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0mcurrent_global_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_STEP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[0;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1157\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Y_4w1ia-99a6",
        "colab_type": "code",
        "outputId": "e39b9524-d825-47f9-b3a9-37bb06900d76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        }
      },
      "cell_type": "code",
      "source": [
        "do_it('eval')\n",
        "#error"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Precision: float32\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f11b6da7898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Waiting for new checkpoint at gs://human-protein-atlas-kaggle/output/\n",
            "INFO:tensorflow:Found new checkpoint at gs://human-protein-atlas-kaggle/output/model.ckpt-39228\n",
            "INFO:tensorflow:Starting to evaluate.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(?, 384), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(?, 1, 1, 384), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-12-01-21:41:11\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-39228\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [4/48]\n",
            "INFO:tensorflow:Evaluation [8/48]\n",
            "INFO:tensorflow:Evaluation [12/48]\n",
            "INFO:tensorflow:Evaluation [16/48]\n",
            "INFO:tensorflow:Evaluation [20/48]\n",
            "INFO:tensorflow:Evaluation [24/48]\n",
            "INFO:tensorflow:Evaluation [28/48]\n",
            "INFO:tensorflow:Evaluation [32/48]\n",
            "INFO:tensorflow:Evaluation [36/48]\n",
            "INFO:tensorflow:Evaluation [40/48]\n",
            "INFO:tensorflow:Evaluation [44/48]\n",
            "INFO:tensorflow:Evaluation [48/48]\n",
            "INFO:tensorflow:Finished evaluation at 2018-12-01-21:43:45\n",
            "INFO:tensorflow:Saving dict for global step 39228: f1 = 0.3725875, global_step = 39228, loss = 0.3037537, precision = 0.3345298, recall = 0.4176459\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 39228: gs://human-protein-atlas-kaggle/output/model.ckpt-39228\n",
            "INFO:tensorflow:Eval results: {'f1': 0.3725875, 'loss': 0.3037537, 'precision': 0.3345298, 'recall': 0.4176459, 'global_step': 39228}. Elapsed seconds: 161\n",
            "INFO:tensorflow:Evaluation finished after training step 39228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NgCutQhuc1B9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "eb21eaa4-9807-4277-9e6f-ac9043071a6b"
      },
      "cell_type": "code",
      "source": [
        "results = do_it('predict')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Precision: float32\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://human-protein-atlas-kaggle/output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f11b46a24e0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Otg-xagt-ar1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "703b6922-1e4b-4894-f38c-4ea7cddfe70f"
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "with file_io.FileIO(DATA_DIR+'sample_submission.csv', 'r') as f:\n",
        "    submit = pd.read_csv(f)\n",
        "    predictions = np.zeros((len(submit), NUM_CLASSES), dtype='float32')\n",
        "    #submit['Predictions']=None #np.zeros(28)\n",
        "    #print(submit.head())\n",
        "for i, r in enumerate(tqdm(results, total=submit.values.shape[0])):\n",
        "    #ids='{0:08x}-{1:04x}-{2:04x}-{3:04x}-{4:012x}'.format(*r['ids'])\n",
        "    #classes = hot_decode(r['predictions'])\n",
        "    #ids = r['ids'].decode('utf8')\n",
        "    #submit[submit.Id==ids].Predicted=classes\n",
        "    #submit[submit.Id==ids].Predictions=[r['predictions']]\n",
        "    predictions[i]=r['predictions']\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/11702 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "Tensor(\"InceptionV3/Logits/Dropout_1b/dropout/mul:0\", shape=(?, 384), dtype=float32)\n",
            "Tensor(\"InceptionV3/Logits/ExpandDims_1:0\", shape=(?, 1, 1, 384), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://human-protein-atlas-kaggle/output/model.ckpt-39228\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 11702/11702 [07:34<00:00, 25.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pFoIuZp1VM5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def hot_decode(pred, thresholds):\n",
        "    results=np.greater(pred, thresholds)\n",
        "    ret=[]\n",
        "    for r in results:\n",
        "        classes=np.nonzero(r)[0]\n",
        "        classes=' '.join([str(c) for c in classes])\n",
        "        ret.append(classes)\n",
        "\n",
        "    return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sHTo8XBRHhDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGFjjFPz8TdP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "def optimize_score(pred, lbl):\n",
        "    pred=np.reshape(pred, (-1))\n",
        "    lbl=np.reshape(lbl.astype('bool'), (-1))\n",
        "    prev=0\n",
        "    curscore=0\n",
        "    thresholds = np.flip(np.unique(pred), axis=0)\n",
        "    with tqdm(thresholds, total=len(thresholds)) as tq:\n",
        "        for i, t in enumerate(tq):\n",
        "            guess=np.greater(pred, t)\n",
        "            curscore = f1_score(lbl, guess)\n",
        "            tq.set_postfix(refresh=False, F1=curscore, Threshold=t)\n",
        "            if curscore !=0 and curscore<=prevscore:\n",
        "                if i == 0:\n",
        "                    print('WTH')\n",
        "                    return 0.2\n",
        "                return (thresholds[i]+thresholds[i-1])/2\n",
        "            prevscore=curscore\n",
        "\n",
        "def predict_eval():\n",
        "    eval_results=do_it('predict', predict_input_fn=vg.input_fn)\n",
        "    print('Building eval results\\n\\n')\n",
        "    eval_predictions=[]\n",
        "    eval_labels=[]\n",
        "    for r in tqdm(eval_results):\n",
        "        ids=r['ids'].decode('utf8')\n",
        "        eval_predictions.append(r['predictions'])\n",
        "        eval_labels.append(r['ds_labels'])\n",
        "    eval_predictions = np.array(eval_predictions)\n",
        "    eval_labels = np.array(eval_labels)\n",
        "    return eval_predictions, eval_labels\n",
        "\n",
        "def tune_thresholds(ep, el):\n",
        "    ep=np.array(ep)\n",
        "    el=np.array(el)\n",
        "    ret = np.zeros(ep.shape[1], dtype='float32')\n",
        "    for i in range(ep.shape[1]):\n",
        "        ret[i]=optimize_score(ep[:,i:i+1], el[:,i:i+1])\n",
        "    return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hiF1NupLgjMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ep, el = predict_eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLuNGpl7EYa1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "a3b3f3ff-2427-4cae-b469-2e1c963f0c66"
      },
      "cell_type": "code",
      "source": [
        "threshs=tune_thresholds(ep, el)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6211 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6213 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6211 [00:00<?, ?it/s]\n",
            "  1%|          | 69/6209 [00:00<00:08, 683.16it/s, F1=0, Threshold=0.0827]\n",
            "  0%|          | 0/6207 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6212 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6209 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6213 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6207 [00:00<?, ?it/s]\n",
            "  2%|         | 146/6205 [00:00<00:08, 724.39it/s, F1=0, Threshold=0.000599]\n",
            "  9%|         | 565/6195 [00:00<00:08, 691.64it/s, F1=0, Threshold=7.46e-6]\n",
            "  0%|          | 0/6209 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6200 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6212 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6211 [00:00<?, ?it/s]\n",
            "  2%|         | 154/6206 [00:00<00:07, 771.48it/s, F1=0, Threshold=0.00544]\n",
            "  0%|          | 0/6208 [00:00<?, ?it/s]\n",
            "  1%|          | 77/6205 [00:00<00:08, 762.09it/s, F1=0, Threshold=0.0119]\n",
            "  0%|          | 0/6213 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6209 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6207 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6210 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6214 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6214 [00:00<?, ?it/s]\n",
            "  1%|          | 76/6207 [00:00<00:08, 751.11it/s, F1=0, Threshold=0.012]\n",
            "  0%|          | 0/6206 [00:00<?, ?it/s]\n",
            "  0%|          | 0/6201 [00:00<?, ?it/s]\n",
            " 35%|      | 2136/6189 [00:03<00:06, 618.93it/s, F1=0, Threshold=8.62e-6]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AGEcQlytH6ZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "1da8b317-208e-4232-f5ac-f0a68e6a86c9"
      },
      "cell_type": "code",
      "source": [
        "print(threshs)\n",
        "\n",
        "def get_full_score(pred, lbl, thresh):\n",
        "    guess = np.greater(pred, thresh)\n",
        "    score = f1_score(lbl, guess, average='macro')\n",
        "    return score\n",
        "get_full_score(ep, el, threshs)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.0027344e-01 4.6807006e-02 1.9293603e-01 8.1810504e-02 6.7299128e-02\n",
            " 2.2262977e-01 8.2112573e-02 1.9353884e-01 9.0935489e-04 5.7950441e-04\n",
            " 7.3542933e-06 4.2244494e-02 2.7607914e-02 5.2148532e-02 4.9903624e-02\n",
            " 5.2134804e-03 2.6390314e-02 1.1886843e-02 8.0088586e-02 7.4295864e-02\n",
            " 7.2091897e-03 1.1465286e-01 7.4306868e-02 2.2637738e-01 1.1067038e-02\n",
            " 3.9346761e-01 9.0792507e-04 8.6065102e-06]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01150460744365135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "metadata": {
        "id": "Msnvmprx5Xz9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit.Predicted=hot_decode(predictions, threshs)\n",
        "#submit.Predicted=hot_decode(predictions, 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QyG3EzUojv-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit.to_csv('inception_rnn.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwJ8fqDWdopP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "3fa22011-4fba-4a36-abae-bbc7a173272b"
      },
      "cell_type": "code",
      "source": [
        "submit[submit.Predicted=='']"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Id, Predicted]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "TVlGiroplQY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}